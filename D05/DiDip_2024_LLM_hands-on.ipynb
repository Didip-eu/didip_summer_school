{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f2303a-e305-4fc3-8139-c1719ae278d8",
   "metadata": {},
   "source": [
    "# A Hands-on Introduction to the Use of LLMs in Digital Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a4575-2600-4dc4-8632-fae523c0c1bc",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d36942-fd9c-41fc-8ab8-79e1a24dc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901184ce-5d05-4a85-a2c1-867131d6ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c1f40-6ebc-469d-ac66-fe83519e47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003ddb8-12f5-4fb0-8deb-4f9cc1ace105",
   "metadata": {},
   "source": [
    "**Note**: After an initial installation of accelerate in Colab, the runtime must be restarted.\n",
    "Sometimes it is also necessary to install the packages a second time after restarting runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578691b7-2d28-4d63-b28e-230aff337f1a",
   "metadata": {},
   "source": [
    "## 1. How to use Large Language Models (LLMs)\n",
    "\n",
    "The simplest (and most widely known) way to interact with high-quality generative AI is through [ChatGPT](https://chatgpt.com/auth/login)\n",
    "\n",
    "**Beware of Data Leakage**: ¬ªWhen you use our services for individuals such as ChatGPT, we may use your content to train our models. You can opt out of training through our privacy portal by clicking on ‚Äúdo not train on my content,‚Äù or to turn off training for your ChatGPT conversations, follow the instructions in our Data Controls FAQ. Once you opt out, new conversations will not be used to train our models.¬´ ([OpenAI FAQs](https://help.openai.com/en/articles/6783457-what-is-chatgpt#))\n",
    "\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "Prompt engineering refers to ¬ªstrategically designing task-specific instructions, \n",
    "refered to as prompts, to guide model output without altering parameters.¬´ \n",
    "([Sahoo et al. 2024: ¬ªA Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications¬´](https://doi.org/10.48550/arXiv.2402.07927))\n",
    "\n",
    "For a detailed collection of common practices, including literature references, see: [Prompt Engineering Guide](https://www.promptingguide.ai/en)\n",
    "\n",
    "**Key Practices**\n",
    "- Write clear instructions\n",
    "    - Ask the model to adapt a persona\n",
    "    - Use delimiters to clearly indicate distinct parts of the input\n",
    "    - Specify the steps rquired to complete a task\n",
    "    - Provide examples\n",
    "    - Specify the desired length of the output\n",
    "- Provide reference text\n",
    "- Split complex tasks into simpler subtasks\n",
    "- Give the model time to ‚Ä∫think‚Äπ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e4428-0d51-4f35-9699-74a1ac7a0b7d",
   "metadata": {},
   "source": [
    "## 2. Hugging Face\n",
    "\n",
    "[Hugging Face](https://huggingface.co) is a company and open-source platform known for its tools and libraries for natural language processing (NLP). It provides easy-to-use APIs and pre-trained models for tasks such as text generation, sentiment analysis, and translation. Hugging Face's Transformers library has become a standard in the NLP community for deploying state-of-the-art models like BERT, T5, an Meta's LLaMa.\n",
    "\n",
    "To get startet you have to sign up on HF and get an access token: https://huggingface.co/join\n",
    "\n",
    "The HF token should be set as a secret in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f45bec-3c83-4afa-bfe4-38aa016181c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This line is only relevant if the notebook is running on your own computer and not on Colab\n",
    "access_token \"...\" #Enter your access token here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e83b0c-5e58-41c2-b4f9-c47d7540f89d",
   "metadata": {},
   "source": [
    "### 2.1 Quickstart\n",
    "\n",
    "OLMo is a series of actually open Language Models designed to enable the science of language models: https://huggingface.co/allenai/OLMo-7B-hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31ef973-8265-444b-b541-7ca0f749b364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994f427-ba4a-4cc5-af2d-a30385872c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_pipe = pipeline(\"text-generation\", model=\"allenai/OLMo-7B-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c03b3b4-21f9-4936-a843-e125877e675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(olmo_pipe(\"Are you an open source Large Language Model?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c662d21-de52-46fb-b518-a26871c04c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c45331-91ff-4c58-a374-0b4fb07c9860",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-7B-hf\") # Loading model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-7B-hf\") # Loading tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140654c-b1ef-4f30-9816-7aecdc98e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\"Are you an opensource Large Language Model?\"]\n",
    "inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n",
    "response = olmo.generate(**inputs, max_new_tokens=512, do_sample=True, top_k=50, top_p=0.95)\n",
    "print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d62fef4-7215-4d80-82fe-c86f4e968905",
   "metadata": {},
   "source": [
    "### 2.2 Why quantization matters\n",
    "\n",
    "Assuming model weights are stored in 32-bit float format:\n",
    "1 model paramater = 4 bytes\n",
    "1 billion paramaters = 4 x 1,000,000,000 bytes = 4GB (not even counting optimizer, gradient and activation info)\n",
    "Many cutting edge models (Llama, GPT4) easily break 100 billion trainable params ü§Ø\n",
    "\n",
    "Let's calculate the size of olmo and the memory it needs for inferences!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cdd825-7334-4a0d-92c2-f3a9dd876b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352cb01-51b5-4a64-8ab5-210f6605ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_param_size(model):\n",
    "    # Calculate the total number of parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    # Convert parameters to a more readable format (e.g., million parameters)\n",
    "    total_params_millions = total_params / 1e6\n",
    "    \n",
    "    print(f\"\"\"The model has {total_params_millions} million parameters.\"\"\")\n",
    "    \n",
    "    return total_params_millions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372872f4-218f-4f23-afe6-d910c6a148d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gpu_memory(model):\n",
    "    # Assuming float32 precision (4 bytes per parameter)\n",
    "    param_memory = sum(p.numel() * 4 for p in model.parameters())\n",
    "    \n",
    "    # Adding a bit of overhead for model structure and intermediate computations\n",
    "    total_memory = param_memory * 1.2\n",
    "    \n",
    "    # Convert bytes to gigabytes\n",
    "    total_memory_gb = total_memory / (1024**3)\n",
    "    \n",
    "    print(f\"\"\"The model requires {total_memory_gb} GB of (GPU-)memory.\"\"\")\n",
    "    \n",
    "    return total_memory_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9fc76c-3939-434f-b616-182591fac45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_params = get_model_param_size(olmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0b418-dcd6-493f-b7a1-4b7d43fb3e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_gpu_needs = estimate_gpu_memory(olmo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3b73da-0df3-421e-9c7f-65c36e3fc606",
   "metadata": {},
   "source": [
    "Solution for the memory problem: **Quantization**\n",
    "\n",
    "Easiest option: huggingface's [bitsandbytes](https://huggingface.co/docs/transformers/main/en/quantization/bitsandbytes)\n",
    "\n",
    "**Note**: Despite the use of bitsandbytes, it may be necessary to restart the runtime after using a model and before using a new one due to the size of the available RAMs and GPUs in Colab. To check the memory usage, click on resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba663f19-145e-42cd-a38c-314f8ee95572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca60c13-7ff9-457b-bec8-23e1cb99f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d4ebc-7a9e-4513-81c9-6e7314b9d2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_quant = AutoModelForCausalLM.from_pretrained(\n",
    "    \"allenai/OLMo-7B-hf\", \n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5e25e4-461e-48f9-a14c-816194fef225",
   "metadata": {},
   "outputs": [],
   "source": [
    "olmo_quant_gpu_needs = estimate_gpu_memory(olmo_quant)\n",
    "print(olmo_quant_gpu_needs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1ef81-ad20-4e46-8868-b01133016cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_olmo(model, tokenizer, question, temperature):\n",
    "    \n",
    "     generate_text = pipeline(\n",
    "         model=model,\n",
    "         tokenizer=tokenizer,\n",
    "         return_full_text=False,  \n",
    "         do_sample=True,\n",
    "         task=\"text-generation\",\n",
    "         max_new_tokens=128,\n",
    "         temperature=temperature\n",
    "     )\n",
    "        \n",
    "    output = generate_text(question)\n",
    "    result = output[0][\"generated_text\"]\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebc589b-9ca4-4d3a-9644-e3f197505ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"...\" #Fill in your question here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a0d9d-84d3-4ed7-a460-31bd78e787c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Fill out to run model with your question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e748e1-f196-421c-9f85-fe77c45e9d4b",
   "metadata": {},
   "source": [
    "### 2.3 Prompt Engineering Playground: Warum Up\n",
    "\n",
    "In this section, you will improve your skills as a prompt engineer. The task is to extract all references to persons from a text excerpt from Hartmann von Aue's Erec."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec461d7-e962-4091-8b97-ca27e47dbcce",
   "metadata": {},
   "source": [
    "Choose one of the following models and complete the code to extract all persons in the text passage:\n",
    "\n",
    "- Llama 3: https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
    "- Vago Solutions: https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\n",
    "\n",
    "To perform this task, you might have to restart the runtime to clear the RAM and GPU memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5b962-cd66-4ba6-a5cc-a1825330ee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run this line if you did not import the huggingface libraries before\n",
    "from transformers import pipline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3695d7-c163-4b9f-ab62-930f39c74667",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Drive HF-Secret\n",
    "from google.colab import userdata\n",
    "access_token = userdata.get('HF_Token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccea68-2f5d-4aa8-9ce0-6c2de4049b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b0282-d401-4c6a-9df8-b67b9ed32fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text snippet from Erec\n",
    "text = \"\"\"\n",
    "n√¢ch der k√ºneginne sage\n",
    "westen die guoten knehte\n",
    "alle vil rehte\n",
    "die z√Æt wenne er solde komen:\n",
    "ouch h√¢ten siz vernomen\n",
    "von dem ritter der d√¢ kam,\n",
    "an dem er den sige nam.\n",
    "diu ros w√¢ren in bereit.\n",
    "d√¥ gen√¥z er s√Æner vr√ºmekeit.\n",
    "mit dem k√ºnege Art√ªse\n",
    "riten von dem h√ªse\n",
    "G√¢wein und Persev√¢us\n",
    "und ein h√™rre genant alsus,\n",
    "der k√ºnec Iels von G√¢l√¥es,\n",
    "und Estorz fil roi Ares,\n",
    "Luc√¢ns der schenke schein in der schar,\n",
    "dar zuo diu massen√Æe gar,\n",
    "daz sin empfiengen alle\n",
    "mit ritterl√Æchem schalle,\n",
    "gesellecl√Æchen unde wol,\n",
    "als man lieben vriunt sol\n",
    "der verlorner vunden ist.\n",
    "gegen im was zer selben vrist\n",
    "√ºber den hof gegangen,\n",
    "daz er w√ºrde empfangen,\n",
    "m√Æn vrouwe diu k√ºneg√Æn.\n",
    "si hiez in willekomen s√Æn:\n",
    "s√Æner √¢ventiure was si vr√¥.\n",
    "vrouwen √än√Æten nam si d√¥,\n",
    "si sprach: \"vrou maget wol get√¢n,\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e32f68-4720-4965-af84-f5ebecf60c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe83154-c534-40fa-b641-4a27c0b2840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = ... #Fill in the name of the choosen model here\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token, use_fast=True)\n",
    "model_16bit = AutoModelForCausalLM.from_pretrained(model_id, token=access_token, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fe748-b990-43f6-b921-eff72f3708c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(model, tokenizer, text, temperature):\n",
    "    \n",
    "    snippet = str(text)\n",
    "    \n",
    "    generate_text = pipeline(\n",
    "             #Complete the function\n",
    "        )\n",
    "    \n",
    "    #Complete this prompt\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "    Text snippet: {snippet}\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = prompt.format(snippet=snippet)\n",
    "    output = generate_text(temp)\n",
    "    result = [output[0][\"generated_text\"]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf276d0-7bbd-45c8-a6b8-232e11d5e304",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_llm() #Complete the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9055a8-b0e8-457a-9275-e075c4256745",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8659fe-28f7-46d2-840a-a334011ef497",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = [\"\"\"Ginov√™r,\n",
    "# unbestimmt (multi), \n",
    "Id√™rs,\n",
    "Art√ªs, \n",
    "G√¢wein,\n",
    "Persev√¢us,\n",
    "Iels von G√¢l√¥es,\n",
    "Estorz,\n",
    "En√Æte\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e767943-350b-42d4-93f0-178be0061929",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function to convert a list of names separated by commas into a list of individual names\n",
    "def split_names(names_list):\n",
    "    # Gehe durch die Liste von Strings und splitte sie an jedem Komma\n",
    "    split_list = [name.strip() for name in names_list[0].split(',')]\n",
    "    return split_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2896a4-97e4-418a-ac2a-0177e98b99a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function for evaluating the results\n",
    "def evaluate_performance(predictions_list, actuals_list):\n",
    "    \n",
    "    prediction_results = []\n",
    "    for item in predictions_list:\n",
    "        if any(string in item for string in actuals_list):\n",
    "            prediction_results.append(True)\n",
    "        else:\n",
    "            prediction_results.append(False)\n",
    "\n",
    "    actual_results = [True] * len(actuals_list)\n",
    "\n",
    "    if len(prediction_results) < len(actual_results):\n",
    "        prediction_results.extend([False] * (len(actual_results) - len(prediction_results)))\n",
    "    if len(actual_results) < len(prediction_results):\n",
    "        actual_results.extend([False] * (len(prediction_results) - len(actual_results)))\n",
    "    \n",
    "    precision = precision_score(actual_results, prediction_results)\n",
    "    recall = recall_score(actual_results, prediction_results)\n",
    "    f1 = f1_score(actual_results, prediction_results)\n",
    "    accuracy = accuracy_score(actual_results, prediction_results)\n",
    "\n",
    "    # Ausgabe der Metriken\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-Score: {f1:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abdc2f-94f3-4834-88b5-dc691c72750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Run helper function if necessary\n",
    "results = ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6d7b99-e171-447d-a167-b1f9be939aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_performance(results, solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1b0cb-f133-4a04-a519-a4d4d3407bbb",
   "metadata": {},
   "source": [
    "## 3. Proprietary LLM: Anthropic's claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab5371-3a4a-41a7-bb27-d74b3bcd325f",
   "metadata": {},
   "source": [
    "Claude is an AI assistant developed by Anthropic. Anthropic is an AI startup company based in San Francisco, California. Anthropic is still headed up by founders but got heavy funding by Google and Amazon: [Anthropic Homepage](https://www.anthropic.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5540d386-71d3-41db-9a17-eef5bc4e15d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559cefc-9df2-4eab-9bf7-d02513495952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c4e8b7-559f-40bf-9585-eed1ff2f1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colab Anthropic Secret\n",
    "ANTHROPIC_API_KEY = userdata.get('secretName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a737bcf9-1b60-4b97-86ed-eef4c77013da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Anthropic Client\n",
    "client = anthropic.Anthropic(\n",
    "    api_key = ANTHROPIC_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ab5b7-bb6f-4718-8dd2-a7e2cb7617ac",
   "metadata": {},
   "source": [
    "#### Prompt Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fdda2d-36fa-4529-bda0-9391b1b95ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(text):\n",
    "    message = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20240620\",\n",
    "    max_tokens=1000,\n",
    "    temperature=0,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": f\"\"\"ENTER YOUR PROMPT HERE {text}\"\"\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ])\n",
    "    return message.content[0].text "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe324e13-7cdb-4121-8a46-ae439f23ec68",
   "metadata": {},
   "source": [
    "#### Prompt Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc49a0-ff80-4102-afc9-cf0812f31602",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_ant(prompt):\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20240620\",\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ])\n",
    "    return message.content[0].text  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40f8c5-3364-48c2-bb1a-64093e2eebc8",
   "metadata": {},
   "source": [
    "### 3.1 Case Study 1: Figure-related Named Entities Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d6e39-c6ca-41dc-b6ff-897d924f4985",
   "metadata": {},
   "source": [
    "In the following, the task from section 2.3 should be solved again, but with a different model (claude-sonnet-3-5) on a larger data set. The goal is to extract all figure- or person-related named entities from Hartmann von Aue's Erec.\n",
    "Before creating or revising your prompt, familiarize yourself with the data frame to be analyzed, which contains the text sections from the Erec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8732c240-908f-4bbc-af9f-021ccbc3fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d39870-5054-4c34-8dc0-b97a4f028efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Google Drive Access\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee1b354-f952-42fc-97a8-caac6273a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Erec Dataframe\n",
    "df = pd.read_json('Erec_GoldAnno.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63138076-db88-4a61-943b-acf23d464aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the length of the dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d17ebf-7c79-44e1-a416-dde78e8e63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the structe of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccbe59-8761-4140-9c35-515736c55483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_NERs(dataframe):\n",
    "    NERs_final = []\n",
    "    combined_gold_tags = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        snippet = row['Text']\n",
    "        text = str(snippet)\n",
    "        gold_tag = (row['NERs'])\n",
    "        \n",
    "        #Complete the prompt\n",
    "        prompt = f\"\"\"\n",
    "       \n",
    "        Middle High German text: {text}\n",
    "        Named Entities:\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        output =  #Complete this line of code. The output should be a list of lists such as [['Figure A', 'Figure B']['Figure A', 'figure C', 'Figure X']]\n",
    "        print(output)\n",
    "        \n",
    "        NERs_final.append(output)\n",
    "        combined_gold_tags.append(gold_tag)\n",
    "    \n",
    "    return NERs_final, combined_gold_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb75b264-7fd8-4f92-9698-5c7a190ce52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to convert a pseudo list of lists, such as [\"['Figure 1', 'Figure 2']\"], into an adequate Python list\n",
    "def convert_sublists(list_of_lists):\n",
    "    converted_list = []\n",
    "    for sublist in list_of_lists:\n",
    "        converted_list.extend([ast.literal_eval(item) for item in sublist])\n",
    "    return converted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6172c9d-ec9d-46d9-969d-34b96f0ff3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(goldstandard, generated):\n",
    "    \"\"\"\n",
    "    Calculates F1, Precision, Recall and Accuracy for two lists of lists.\n",
    "\n",
    "    :param goldstandard: List of lists containing the gold standard data.\n",
    "    :param generated: List of lists containing the generated data.\n",
    "    :return: A dictionary containing the calculated metrics.\n",
    "    \"\"\"\n",
    "    total_tp = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "    total_elements = 0\n",
    "\n",
    "    for i in range(len(goldstandard)):\n",
    "        gold_sublist = set(goldstandard[i])\n",
    "        if i < len(generated):\n",
    "            generated_sublist = set(generated[i])\n",
    "        else:\n",
    "            generated_sublist = set()\n",
    "\n",
    "        tp = gold_sublist.intersection(generated_sublist)\n",
    "        fp = generated_sublist - gold_sublist\n",
    "        fn = gold_sublist - generated_sublist\n",
    "\n",
    "        total_tp += len(tp)\n",
    "        total_fp += len(fp)\n",
    "        total_fn += len(fn)\n",
    "        total_elements += len(gold_sublist)\n",
    "\n",
    "    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = total_tp / total_elements if total_elements > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'F1': f1,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'Accuracy': accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40349e5b-f160-4fcf-ab0b-f77f505b128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Named Entites from dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a869f-9742-453e-9260-d77a1dc90740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list of lists if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837a793-7c77-4086-92a2-02dcc06393cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate your output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c186b85-4f3d-475b-a242-0dd17c64fd66",
   "metadata": {},
   "source": [
    "### Optional: Figure-related Named Entities Recognition with a HuggingFace Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f31ac-6495-4c72-bdea-c9b99743cc8f",
   "metadata": {},
   "source": [
    "Below you can check whether you get better scores with a HuggingFace model than with the Anthropic model. This task is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f44a4-da1b-48a4-b700-225ab78e8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab4dd3-4a6b-404e-9cb6-4611a23b052c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only run the following lines if you did not import the huggingface libraries before\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e85810-be97-4772-9c16-9e0a6b2628a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82911bf-5e91-4439-be70-531eee87bc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model\n",
    "#Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d3d373-5e25-47d2-bd25-d61ca173f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm(model, tokenizer, prompt, temperature):\n",
    "    generate_text = pipeline(\n",
    "    #Complete this function\n",
    "    )\n",
    "    output = generate_text(prompt)\n",
    "    result = output[0][\"generated_text\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b27887-cd6f-4e4b-93fb-e956a9d7e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that iterates over a column of a data frame and extracts the NERs using an LLM\n",
    "def extract_NERs(model, tokenizer, dataframe, temperature):\n",
    "    NERs_final = []\n",
    "    combined_gold_tags = []\n",
    "    \n",
    "    for index, row in dataframe.iterrows():\n",
    "        snippet = row['Text']\n",
    "        gold_tag = (row['NERs'])\n",
    "        \n",
    "        template = f\"\"\"\n",
    "        COMPLETE THIS PROMPT\n",
    "        {snippet}\n",
    "        \"\"\"\n",
    "        \n",
    "        prompt = template.format(snippet=snippet)\n",
    "        result = run_llm(model, tokenizer, prompt, temperature)\n",
    "        #print(result) -> Comment this line if you want to see the output\n",
    "        NERs_final.append(result)\n",
    "        combined_gold_tags.append(gold_tag)\n",
    "    \n",
    "    combined_outputs = zip(NERs_final, combined_gold_tags)\n",
    "    return combined_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bbcc1-b061-4713-9537-30a44ba6bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs = #Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe407b8-ab96-40d3-a14b-cdabddd9463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = #Complete this function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
