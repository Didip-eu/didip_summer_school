# Day 5: Future Directions (LLM)

Axel Pichler (Univ. Stuttgart): Large Language Models
- with Georg Vogeler
- This concluding session will discuss the future directions of language models, particularly large language models (LLMs), emphasizing their evolving capabilities, ethical considerations, and potential impacts on various industries. Participants will explore cutting-edge developments, such as generative adversarial networks and reinforcement learning from human feedback, and discuss how these innovations could shape the next generation of AI applications.

## You will learn

- How to use Large Language Models (LLMs): You will explore different ways to interact with high-quality generative AI models, including the widely known ChatGPT. Additionally, you will learn about the importance of prompt engineering and how to effectively craft prompts to guide the model's output.
- Hugging Face and Transformers: You will be introduced to the Hugging Face platform and the Transformers library, which have become standard tools in the NLP community for deploying state-of-the-art models like BERT, T5, and Meta's LLaMA.
- Quantization and Memory Management: You will learn about the importance of model quantization and its impact on memory usage, and explore techniques to optimize the deployment of LLMs on limited hardware resources.
- Prompt Engineering Playground: You will have the opportunity to practice your prompt engineering skills by extracting all references to persons from a text excerpt from Hartmann von Aue's Erec using different LLM models.
- Anthropic's Claude: You will be introduced to Anthropic's proprietary LLM, Claude, and learn how to use it for various tasks, such as named entity recognition on a larger dataset from the Erec text.
- Optional: Named Entity Recognition with HuggingFace: As an optional exercise, you can explore whether you can achieve better performance on the named entity recognition task using a HuggingFace model compared to the Anthropic model.

Throughout the day, you will work with both open-source and proprietary LLMs, gaining hands-on experience in deploying and evaluating these models in the context of Digital Humanities research and analysis.
