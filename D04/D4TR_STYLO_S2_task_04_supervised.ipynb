{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "268ecd03-c139-4090-a3c4-90526a2dde66",
      "metadata": {
        "id": "268ecd03-c139-4090-a3c4-90526a2dde66"
      },
      "source": [
        "# 4. Supervised Analysis and Classifiers\n",
        "\n",
        "Supervised algorithms (or **supervised** models) observe the way in which the feature inputs `X` (e.g. function word frequencies of known texts) correlate with class outputs `y` (e.g. authorship / label of the text). A supervised model ‘observes’ correctly labelled, preclassified X-y pairs, in order to register meaningful correlations (e.g. between certain function word patterns X with certain authors y). This process is called ‘training,’ and the X-y pairs which the model trains on correspond to what is commonly referred to as `training data`. Consequently, once this learning process has taken place, the model can be confronted with `test data,` comprising previously unobserved and unclassified texts. On the basis of what it has observed, the supervised model can make a prediction (classification), and assign the unseen test data to a class, either by a hard decision or by outputting a probability score.\n",
        "\n",
        "Classification is a considerable field of research on its own. There are **many types** of classifiers, and it is not always clear which one will perform best, and why. Varying types of classifiers tend to react differently to different problems, have a variety of parametrization options and require other methods by which to optimize their performance during training. A big advantage of supervised machine-learning methods, ‘text classification’ (Sebastiani 2002), is the **possibility of evaluation**. By making different combinations of parameters, such as the feature set, the vector length (number of features), sample length, vectorization method, scaling method, etc., and evaluating how well they can be fitted to a class (author), scholars can finetune and optimize these parameters.\n",
        "Before we proceed, we **repeat**, with the block of code below, some of **the steps from the previous notebook**.\n",
        "\n",
        "These are:\n",
        "\n",
        "1. Loading and segmentation of documents, containers `authors`, `titles`, `texts`\n",
        "2. Vectorization of `texts` to matrix `X` containing vectors for all text segments\n",
        "3. Scale `X` by applying `StandardScaler()`\n",
        "\n",
        "Note that, as opposed to the three previous notebooks, we are now introducing a **test corpus** of allegedly **unknown authorship**. These files can be found in our `'corpus/test/'` folder. Below, we will apply a classifier to attribute the text to one of the **known classes**, i.e. our training set from the `'corpus/train/'` folder."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change working directory\n",
        "%cd /content/drive/MyDrive/didip_ss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM29LEe4-Wtt",
        "outputId": "c493cd51-925e-4b4b-93ae-0ecff1b4f665"
      },
      "id": "bM29LEe4-Wtt",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive/didip_ss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b7fa8ceb-1f6f-4bff-b015-2ed6eae05410",
      "metadata": {
        "id": "b7fa8ceb-1f6f-4bff-b015-2ed6eae05410"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from string import punctuation\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "current_directory = os.getcwd() # gets current directory\n",
        "folder_path = 'D04/corpus' # gets directory path to corpus folder containing .txt files\n",
        "\n",
        "# We declare some parameters — the 'settings' of our stylometric experiments\n",
        "sample_len = 5000 # word length of text segment\n",
        "\n",
        "data_dict = {}\n",
        "for folder_name in glob.glob(folder_path + '/*'):\n",
        "\n",
        "    dict_key = folder_name.split('/')[-1] # make train and test data split\n",
        "\n",
        "    # Declare empty lists to fill up with our metadata and data\n",
        "    authors = []\n",
        "    titles = []\n",
        "    texts = []\n",
        "\n",
        "    # Open all file objects in folder and gather data\n",
        "    for filename in glob.glob(folder_name + '/*'):\n",
        "        author = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[0]\n",
        "        title = filename.split(\"/\")[-1].split(\".\")[0].split(\"_\")[1]\n",
        "\n",
        "        bulk = []\n",
        "        text = open(filename, encoding='utf-8-sig').read() # utf-8-sig encoding automatically handles and removes Unicode Byte Order Mark (BOM) if present\n",
        "\n",
        "        # .split() method splits string into list of substrings based on a specified delimiter. By default, the delimiter is a whitespace\n",
        "        # .strip() method removes leading and trailing whitespace from a string: spaces, tabs, newlines, and other whitespace characters.\n",
        "        for word in text.strip().split():\n",
        "            word = re.sub('\\d+', '', word) # escape digits\n",
        "            word = re.sub('[%s]' % re.escape(punctuation), '', word) # escape punctuation\n",
        "            word = word.lower() # convert upper to lowercase\n",
        "            bulk.append(word)\n",
        "\n",
        "        # Split up the text into discrete chunks or segments\n",
        "        bulk = [word for word in bulk if word != \"\"] # list comprehension that removes emptry strings\n",
        "        bulk = [bulk[i:i+sample_len] for i in range(0, len(bulk), sample_len)]\n",
        "        for index, sample in enumerate(bulk):\n",
        "            if len(sample) == sample_len:\n",
        "                authors.append(author)\n",
        "                titles.append(title + \"_{}\".format(str(index + 1)))\n",
        "                texts.append(\" \".join(sample))\n",
        "\n",
        "    data_dict[dict_key] = [authors, titles, texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b7976688-fa6a-45da-8d77-e16fb7883b05",
      "metadata": {
        "id": "b7976688-fa6a-45da-8d77-e16fb7883b05"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# In this block of code, we separately vectorize and scale the train and test partitions of our dataset\n",
        "\n",
        "for set, [authors, titles, texts] in data_dict.items():\n",
        "    if set == 'train': # make sure your folder is called 'train'!\n",
        "        # Vectorize by most common words\n",
        "        model = CountVectorizer(max_features=250, # n features = vector length / vector dimensionality.\n",
        "                                analyzer='word', # feature type\n",
        "                                ngram_range=((1,1)))\n",
        "        X_train = model.fit_transform(texts).toarray()\n",
        "\n",
        "        feat_frequencies = np.asarray(X_train.sum(axis=0)).flatten()\n",
        "        features = model.get_feature_names_out()\n",
        "        feat_freq_df = pd.DataFrame({'feature': features, 'frequency': feat_frequencies})\n",
        "        feat_freq_df = feat_freq_df.sort_values(by='frequency', ascending=False).reset_index(drop=True)\n",
        "        sorted_features = feat_freq_df['feature'].tolist()\n",
        "        sorted_indices = [model.vocabulary_[feat] for feat in sorted_features]\n",
        "        X_train_sorted = X_train[:, sorted_indices]\n",
        "\n",
        "        # Feed sorted features again to new model\n",
        "        model = CountVectorizer(stop_words=[],\n",
        "                                analyzer='word',\n",
        "                                vocabulary=sorted_features,\n",
        "                                ngram_range=((1,1)))\n",
        "        X_train = model.fit_transform(texts).toarray()\n",
        "\n",
        "        # Scale by StandardScaler()\n",
        "        scaler = StandardScaler()\n",
        "        X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        y_train = le.fit_transform(authors)\n",
        "\n",
        "# For vectorizing and scaling our test set, we use the same models as for our training set!\n",
        "for set, [authors, titles, texts] in data_dict.items():\n",
        "    if set == 'test': # make sure your folder is called 'test'!\n",
        "        X_test = model.transform(texts).toarray()\n",
        "        X_test = scaler.transform(X_test)\n",
        "        test_titles = titles"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "599e2a61-9f27-442f-8c83-1d831ecbea7d",
      "metadata": {
        "id": "599e2a61-9f27-442f-8c83-1d831ecbea7d"
      },
      "source": [
        "## 4.1 Training a Classifier (by applying SVM)\n",
        "\n",
        "Especially in recent years, that have witnessed the rise of machine learning and computing power,  classification algorithms such as support vector machines (SVM’s, **support vector machines**) have become increasingly popular. A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal hyperplane that maximally separates data points of different classes in a high-dimensional space. SVM is effective in high-dimensional spaces and is versatile with different kernel functions for non-linear classification.\n",
        "\n",
        "For now, we will not occupy ourselves too much with the hyperparameters of SVM's just yet, and first take a look at some more general principles of training and evaluating classifiers.\n",
        "\n",
        "### 4.1.1 Preparing the Dataset for Training → `train_test_split`\n",
        "\n",
        "Above, we already declared a train and test set in separate folders. During training, however, it is considered good practice to hold out a **development set** (`X_dev`), also known as a **validation set**. It is a subset of the training set that is set aside during the training of a machine learning model. This heldout `X_dev` is not used in the training process but is instead used to evaluate the model's performance during development. The heldout dev set allows you to assess how well your model is likely to perform on unseen data (i.e. `X_test`), providing a better estimate of its generalization ability, offering a basis for tuning hyperparameters by evaluating different settings, detecting overfitting early to take preventive measures, and aiding in selecting the best performing model for production.\n",
        "\n",
        "Here is a list of the variables that you will encounter in the process of partitioning our data set:\n",
        "\n",
        "* `X_train` and `y_train`: The **full training set**: all vectorized text segments (`X_train`) labelled by authorship (`y_train`).\n",
        "* `X_train_split` and `y_train_split`: The **remaining training set** after subtraction of the validation set.\n",
        "* `X_dev` and `y_dev`: The **validation set**, subset of the training data  temporarily held out in order to function as a kind of stand-in test set.\n",
        "* `X_test`:  The *actual* **test set**, i.e. texts unseen by the model, for which authorship are —truly, this time— unknown.\n",
        "\n",
        "Parameters to take into account when subtracting `X_dev` from `X_train` are the **split ratio** (`test_size=0.33`) and a **random seed** in the split process to ensure that the results are reproducible (`random_state`).\n",
        "\n",
        "### 4.1.2 Evaluation: Accuracy, Precision, Recall, F1 score\n",
        "\n",
        "Once our model is trained on `X_train_split` and known labels `y_train_split`, we are effectively able to test the model's quality by having it predict on the heldout `X_dev` set. This yields a vector of **predictions** `y_dev_pred`, which can be readily compared to what is commonly referred to as \"**ground truth**\" or \"**gold standard**\".\n",
        "\n",
        "During training, each of our authors (let's say for now, authors A, B, and C) is awarded a class label corresponding to a digit, e.g. `0`, `1`, `2`.\n",
        "- The `y_dev`-array will, therefore, look like something like this: `[0, 0, 1, 1, 2, 2]`, where each of 3 authors in the training set is corresponded by 2 text samples in the development set.  \n",
        "- Possibly, our model can output as prediction (`y_dev_pred`) the vector array `[0, 1, 1, 1, 2, 2]`.\n",
        "\n",
        "Clearly, it has made a mistake in misattributing the second text segment to class `1` (Author B) instead of class `0` (Author A).\n",
        "\n",
        "When comparing the golden standard against the predictions, we can extract several interesting evaluation metrics from a `classification_report`, yielding `accuracy`, `precision`, `recall`,`f1-score`.\n",
        "\n",
        "* `accuracy`: *\"How often did we correctly attribute the text segment to a given author?\"*  \n",
        "  I.e. the percentage of correct predictions out of all the predictions made. The answer in our case may be obvious: 5 out of 6 times, 0.83.\n",
        "* `precision`: *\"When we positively identified a text segment as written by a given author, how often was that true?\"*  \n",
        "  Precision is calculated for each class separately and later averaged across classes. In our case, let us consider the example of Author B. In case of Authors A and C, the precision is in fact 100% in both cases: all positive identifications (1/1 for Author A and 2/2 for Author B) were indeed positive. In case of Author B, however, at one time `1` flared up where the outcome should have been `0` (= Author A). This is a **false positive**, and impairs our model's precision to 2/3 —out of 3 positive outcomes for Author B, only 2 were in fact correct—, and yields a score of 0.67. On average, our precision (when macro-averaged across all classes) is 0.89.\n",
        "* `recall`: *\"Were we able to attribute all text segments of a given author to that author?\"*  \n",
        "  Recall computes how many out of all predictions that should have been labelled positive were actually labelled such. Again, each class is first looked at individually. It turns out that, when we indeed look at the example of Author A, we in fact only caught half of the `0`'s we should have caught, because we falsely attributed an observation belonging to `0` to class `1`. The missed instance for class `0` is what we call a **false negative**, and impairs the recall for that class to 1/2, i.e. 0.5. For Authors B and C, the results are 2/2 and 2/2, both times 100%. Taken on average, then, when looking at our model's performance on all authors, the so-called macro recall adds up to 0.83.\n",
        "* `f1-score`: A balanced score (harmonic mean) that combines precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "09daf3e9-bda5-4892-bc01-04adb97ec950",
      "metadata": {
        "id": "09daf3e9-bda5-4892-bc01-04adb97ec950",
        "outputId": "90ac6700-bede-4d26-bd60-e56218085b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensions of original data set:\n",
            "(64, 250)\n",
            "Dimensions of partitions (train and dev) set:\n",
            "(42, 250)\n",
            "(22, 250)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         5\n",
            "           1       1.00      1.00      1.00        11\n",
            "           2       1.00      1.00      1.00         6\n",
            "\n",
            "    accuracy                           1.00        22\n",
            "   macro avg       1.00      1.00      1.00        22\n",
            "weighted avg       1.00      1.00      1.00        22\n",
            "\n",
            "\n",
            "Predicted authorship:\n",
            "                                     Prediction\n",
            "Liber-vitae-meritorum_1   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_2   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_3   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_4   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_5   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_6   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_7   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_8   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_9   Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_10  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_11  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_12  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_13  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_14  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_15  Hildegardis-Bingensis\n",
            "Liber-vitae-meritorum_16  Hildegardis-Bingensis\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "\n",
        "\"\"\"\n",
        "Train on partitioned X_train_split, y_train_split\n",
        "Test on validation set X_dev => yields y_dev_pred\n",
        "\"\"\"\n",
        "\n",
        "# Splits datasets into random train and test subsets.\n",
        "X_train_split, X_dev, y_train_split, y_dev = train_test_split(X_train, y_train, test_size=0.33, random_state=1) # test_size: 1/3 of train data becomes validation set\n",
        "\n",
        "print('Dimensions of original data set:')\n",
        "print(X_train.shape)\n",
        "print('Dimensions of partitions (train and dev) set:')\n",
        "print(X_train_split.shape)\n",
        "print(X_dev.shape)\n",
        "\n",
        "# Initialize an SVM-classifier\n",
        "svm_classifier = SVC(kernel='linear', C=1.0, random_state=42) # random seed ensures reproducibility\n",
        "svm_classifier.fit(X_train_split, y_train_split)\n",
        "\n",
        "# Make predictions with model\n",
        "y_dev_pred = svm_classifier.predict(X_dev) # y_pred = model predictions\n",
        "\n",
        "print(classification_report(y_dev, y_dev_pred)) # compare predictions to ground truth / gold standard\n",
        "\n",
        "\"\"\"\n",
        "Test on test set\n",
        "Yields y_pred (predictions) of authorship\n",
        "\"\"\"\n",
        "\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "predictions = le.inverse_transform(y_pred)\n",
        "\n",
        "print()\n",
        "print(\"Predicted authorship:\")\n",
        "\n",
        "df = pd.DataFrame(predictions) # structures matrix X as a DataFrame\n",
        "df.columns = ['Prediction'] # assigns column labels\n",
        "df.index = test_titles\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54105e5a-bed2-4b56-b360-66bfdb5fee78",
      "metadata": {
        "id": "54105e5a-bed2-4b56-b360-66bfdb5fee78"
      },
      "source": [
        "## 4.2 `GridsearchCV()`: Tuning Parameters and Hyperparameters of the SVM (Advanced)\n",
        "\n",
        "In this section, we mainly repeat much of the above, but introduce the useful class `sklearn.model_selection.GridSearchCV`.\n",
        "Think of what follows as a more advanced and specialized way of going about training your model. This time, we do not simply *choose* whatever parameters we think will work best, we statistically analyze and evaluate a series of varying presets, in order to gauge their performance on a more objective basis.\n",
        "\n",
        "An SVM has quite a few hyperparameters, such as the regularization parameter (`'C'`) and the kernel parameters (like `'linear'`).\n",
        "Moreover, from a stylometric methodological perspective, we may want to experiment with varying feature types (function words, character n-grams, ...), feature vector lengths (`n_features`), and segment lengths (`sample_len`). Gridsearch can help us find the optimal settings, ensuring the SVM model achieves the highest possible accuracy and generalizes well to unseen data. This process helps to avoid the pitfalls of manual tuning (which can be subjective) and ensures a more robust and reliable model.\n",
        "\n",
        "Below, we first declare these various presets in containers, e.g. `sample_len_loop`, `feat_type_loop`, `feat_n_loop`, `c_options`, `kernel_options`, and `k_folds`.\n",
        "\n",
        "**READ FIRST: Searching many parameters at the same time can be quite costly and take a long time. Try to start your gridsearch by focussing on only a few of these parameters at a time, just so you can get acquainted with them.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22",
      "metadata": {
        "id": "bc7931aa-9e0e-4d6f-8c7e-dbf8457c2b22"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import f1_score, make_scorer, recall_score, accuracy_score, precision_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import Normalizer, StandardScaler, FunctionTransformer, LabelBinarizer, MinMaxScaler\n",
        "from string import punctuation\n",
        "import glob\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "\n",
        "current_directory = os.getcwd() # gets current directory\n",
        "\n",
        "# preprocessing, feature type and vectorization params\n",
        "sample_len_loop = [1000, 2000, 3000]\n",
        "feat_type_loop = ['raw_MFW','raw_4grams','tfidf_MFW','tfidf_4grams']\n",
        "feat_n_loop = [250, 500, 750, 1000]\n",
        "\n",
        "# SVM model and training params\n",
        "c_options = [1, 10, 100, 1000] # various C-parameters - adjust the decision margin's flexibility.\n",
        "kernel_options = ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "k_folds = [3, 5, 7, 10]\n",
        "\n",
        "# Function to turn sparse into dense matrices\n",
        "def to_dense(X):\n",
        "        X = X.todense()\n",
        "        X = np.asarray(X) # Because TypeError: np.matrix is not supported.\n",
        "        X = np.nan_to_num(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e234161-8611-4682-bb15-dbfdfa2c602c",
      "metadata": {
        "id": "8e234161-8611-4682-bb15-dbfdfa2c602c"
      },
      "source": [
        "Consequently, by using `GridSearchCV()`, one can efficiently navigate through the classifier's parameter space. Moreover, we expand its functionality in the code to follow by introducing a number of implementations particularly suitable for non-traditional authorship attribution. The code block below is an example.\n",
        "\n",
        "1. **Preprocessing**: First, we open and preprocess our files again (a step you are familiar with by now), and store our `texts` (text segments by a certain `sample_len`) in a variable `X_train`. Consequently, we label the segments by authorship and store the labels in `y_train` (if there are three authors, the labels should be `0` for author A, `1` for author B, and `2` for author C).\n",
        "2. **Vectorization**: We initialize various vectorization options, where we take into account the feature type (`word` or `char`), declare our `n_gram` preferences, and decide whether we want to input raw frequencies (`CountVectorizer()`) or else TF-IDF frequencies (`TfidfVectorizer`).\n",
        "3. **Pipeline and Parameter Grid**: We build a `pipe` (`Pipeline`) a tool for chaining together these data preprocessing steps and machine learning algorithms into a single object. This enables seamless and efficient handling of data transformation and model training, facilitating the creation of end-to-end machine learning workflows. We store our variables in a dictionary `param_grid`, specifying the hyperparameter values to search over during the grid search. It allows for systematic exploration of different combinations of hyperparameters to identify the optimal configuration for a machine learning model.\n",
        "4. **Grid Search and Cross-Validated Results**: Finally, we introduce a new important concept, that of cross-Validation (CV). In fact, CV is an advanced and more reliable way of going about `train_test_split` as it was introduced in the block of code earlier. With CV, the dataset is divided into *k* equal-sized **folds**. Consequently, the model is trained *k* times, where each time it is trained on *k*−1 folds (which corresponds to `X_train_split` above) and tested on the remaining fold (`X_dev`). The performance metrics (accuracy, precision, recall, f1-score) are averaged over the *k* trials to give a more reliable estimate of the model's performance (information you can extract from `results['mean_test_accuracy_score']`).\n",
        "\n",
        "This helps ensure that the model is not overly dependent on a particular subset of the data, as well as provides a more accurate estimate of the model’s performance on unseen data.\n",
        "\n",
        "Try to tweak the various parameter settings above, and then run the code below.\n",
        "\n",
        "**Searching many parameters at the same time can be quite costly and take a long time. Try to start your gridsearch by focussing on only a few of these parameters at a time, just so you can get acquainted with them.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import glob\n",
        "import re\n",
        "from string import punctuation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score, accuracy_score\n",
        "import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import multiprocessing\n",
        "import scipy.sparse as sp\n",
        "\n",
        "# Debug flag\n",
        "DEBUG = True\n",
        "\n",
        "# Early stopping parameters\n",
        "N_ITER_NO_CHANGE = 5\n",
        "TOLERANCE = 0.001\n",
        "\n",
        "# Function for debug logging\n",
        "def debug_log(message):\n",
        "    if DEBUG:\n",
        "        print(f\"DEBUG: {message}\")\n",
        "\n",
        "folder_directory ='D04/corpus/train'\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_file(filename, sample_len):\n",
        "    author, title = filename.split('/')[-1].split('.')[0].split('_')[:2]\n",
        "    with open(filename, encoding='utf-8-sig') as file:\n",
        "        text = file.read().strip()\n",
        "        words = re.sub(r'[\\d%s]' % re.escape(punctuation), '', text.lower()).split()\n",
        "        bulk = [words[i:i + sample_len] for i in range(0, len(words), sample_len)]\n",
        "        return [(author, f\"{title}_{index + 1}\", \" \".join(sample)) for index, sample in enumerate(bulk) if len(sample) == sample_len]\n",
        "\n",
        "# Function to convert sparse matrix to numpy array\n",
        "def to_numpy_array(X):\n",
        "    if sp.issparse(X):\n",
        "        return X.toarray()\n",
        "    return np.asarray(X)\n",
        "\n",
        "# Main processing function\n",
        "def process_parameters(feat_type, n_feats, sample_len, k_folds, X_train, y_train):\n",
        "    try:\n",
        "        vectorizer_params = {\n",
        "            'raw_MFW': (CountVectorizer, 'word', (1, 1)),\n",
        "            'tfidf_MFW': (TfidfVectorizer, 'word', (1, 1)),\n",
        "            'raw_4grams': (CountVectorizer, 'char', (4, 4)),\n",
        "            'tfidf_4grams': (TfidfVectorizer, 'char', (4, 4))\n",
        "        }\n",
        "\n",
        "        vectorizer_class, analyzer, ngram_range = vectorizer_params[feat_type]\n",
        "        vectorizer = vectorizer_class(analyzer=analyzer, ngram_range=ngram_range, max_features=n_feats)\n",
        "\n",
        "        pipe = Pipeline([\n",
        "            ('vectorizer', vectorizer),\n",
        "            ('to_dense', FunctionTransformer(to_numpy_array, accept_sparse=True)),\n",
        "            ('feature_scaling', StandardScaler()),\n",
        "            ('classifier', svm.SVC(probability=True))\n",
        "        ])\n",
        "\n",
        "        param_grid = [{\n",
        "            'vectorizer': [vectorizer],\n",
        "            'feature_scaling': [StandardScaler()],\n",
        "            'classifier__C': c_options,\n",
        "            'classifier__kernel': kernel_options,\n",
        "        }]\n",
        "\n",
        "        grid = GridSearchCV(pipe, cv=k_folds, n_jobs=-1, param_grid=param_grid,\n",
        "                            scoring={\n",
        "                                'precision_score': make_scorer(precision_score, labels=y_train, average='macro'),\n",
        "                                'recall_score': make_scorer(recall_score, labels=y_train, average='macro'),\n",
        "                                'f1_score': make_scorer(f1_score, labels=y_train, average='macro'),\n",
        "                                'accuracy_score': make_scorer(accuracy_score),\n",
        "                            },\n",
        "                            refit='f1_score',\n",
        "                            verbose=False,\n",
        "                            error_score='raise'  # This will raise the error instead of returning a warning\n",
        "                        )\n",
        "\n",
        "        grid.fit(X_train, y_train)\n",
        "        results = grid.cv_results_\n",
        "\n",
        "        return (feat_type, n_feats, sample_len, k_folds, results)\n",
        "    except Exception as e:\n",
        "        debug_log(f\"Error in process_parameters: {str(e)}\")\n",
        "        return (feat_type, n_feats, sample_len, k_folds, None)\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    all_grid_scores = []\n",
        "    all_parameter_combos = []\n",
        "\n",
        "    try:\n",
        "        # Preprocess all files\n",
        "        all_files = glob.glob(folder_directory + '/*')\n",
        "        max_sample_len = max(sample_len_loop)  # Assume sample_len_loop is defined\n",
        "\n",
        "        with Parallel(n_jobs=-1) as parallel:\n",
        "            all_samples = parallel(delayed(preprocess_file)(filename, max_sample_len) for filename in all_files)\n",
        "\n",
        "        all_samples = [item for sublist in all_samples for item in sublist]\n",
        "        authors, titles, texts = zip(*all_samples)\n",
        "\n",
        "        debug_log(f\"Preprocessed {len(texts)} samples\")\n",
        "\n",
        "        X_train = texts\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_train = label_encoder.fit_transform(authors)\n",
        "\n",
        "        debug_log(f\"Prepared X_train with {len(X_train)} samples and y_train with {len(y_train)} labels\")\n",
        "\n",
        "        # Create parameter combinations\n",
        "        param_combinations = [(feat_type, n_feats, sample_len, k)\n",
        "                              for feat_type in feat_type_loop\n",
        "                              for n_feats in feat_n_loop\n",
        "                              for sample_len in sample_len_loop\n",
        "                              for k in k_folds]\n",
        "\n",
        "        # Run grid search in parallel\n",
        "        with Parallel(n_jobs=-1) as parallel:\n",
        "            results = parallel(delayed(process_parameters)(feat_type, n_feats, sample_len, k, X_train, y_train)\n",
        "                               for feat_type, n_feats, sample_len, k in tqdm.tqdm(param_combinations, desc=\"Grid Search Progress\"))\n",
        "\n",
        "        # Process results\n",
        "        best_score = -np.inf\n",
        "        for feat_type, n_feats, sample_len, k, grid_results in results:\n",
        "            if grid_results is None:\n",
        "                continue\n",
        "            current_best_score = grid_results['mean_test_f1_score'].max()\n",
        "\n",
        "            if current_best_score > best_score + TOLERANCE:\n",
        "                best_score = current_best_score\n",
        "\n",
        "                params = grid_results['params']\n",
        "                c_settings = [i['classifier__C'] for i in params]\n",
        "                kernel_settings = [i['classifier__kernel'] for i in params]\n",
        "                k_fold_settings = [k for _ in params]\n",
        "                feat_type_settings = [feat_type for _ in params]\n",
        "                n_feat_settings = [n_feats for _ in params]\n",
        "                sample_len_settings = [sample_len for _ in params]\n",
        "\n",
        "                accuracies = grid_results['mean_test_accuracy_score']\n",
        "                precisions = grid_results['mean_test_precision_score']\n",
        "                recalls = grid_results['mean_test_recall_score']\n",
        "                f1_scores = grid_results['mean_test_f1_score']\n",
        "\n",
        "                for evaluations in zip(accuracies, precisions, recalls, f1_scores):\n",
        "                    all_grid_scores.append(evaluations)\n",
        "\n",
        "                for parameter_combo in zip(c_settings, kernel_settings, k_fold_settings, feat_type_settings, n_feat_settings, sample_len_settings):\n",
        "                    all_parameter_combos.append(parameter_combo)\n",
        "\n",
        "        # Create and save report\n",
        "        full_report = []\n",
        "        for (acc, prec, recall, f1), params in zip(all_grid_scores, all_parameter_combos):\n",
        "            model_name = '-'.join([str(i) for i in params])\n",
        "            full_report.append((model_name, acc, prec, recall, f1))\n",
        "\n",
        "        df = pd.DataFrame(full_report, columns=['model', 'accuracy', 'precision', 'recall', 'f1 score'])\n",
        "        df_sorted = df.sort_values(by='f1 score', ascending=False)\n",
        "        print(df_sorted)\n",
        "\n",
        "        debug_log(\"Dataframe created and sorted\")\n",
        "\n",
        "        current_time = datetime.now()\n",
        "        formatted_time = current_time.strftime(\"date %d-%m at %Hh%Mm\")\n",
        "        df_sorted.to_excel(f'D04/output/gridsearch_evaluation_report-{formatted_time}.xlsx', index=False)\n",
        "\n",
        "        debug_log(f\"Results written to Excel file: gridsearch_evaluation_report-{formatted_time}.xlsx\")\n",
        "\n",
        "    except Exception as e:\n",
        "        debug_log(f\"An error occurred: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXMazcsWJyhM",
        "outputId": "6a3ed274-2f42-48c5-ae0b-0c22f5095d03"
      },
      "id": "BXMazcsWJyhM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Preprocessed 109 samples\n",
            "DEBUG: Prepared X_train with 109 samples and y_train with 109 labels\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "Grid Search Progress:   0%|          | 0/192 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   2%|▏         | 4/192 [00:47<37:11, 11.87s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   3%|▎         | 6/192 [02:41<1:35:00, 30.65s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   4%|▍         | 8/192 [04:04<1:46:29, 34.73s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   5%|▌         | 10/192 [05:54<2:06:28, 41.69s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   6%|▋         | 12/192 [07:38<2:15:41, 45.23s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   7%|▋         | 14/192 [09:34<2:26:22, 49.34s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   8%|▊         | 16/192 [10:49<2:13:39, 45.57s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:   9%|▉         | 18/192 [12:40<2:20:58, 48.61s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  10%|█         | 20/192 [14:16<2:18:53, 48.45s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  11%|█▏        | 22/192 [16:04<2:22:01, 50.13s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  12%|█▎        | 24/192 [17:11<2:06:24, 45.14s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  14%|█▎        | 26/192 [18:52<2:09:05, 46.66s/it]\u001b[A\u001b[A/usr/local/lib/python3.10/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "Grid Search Progress:  15%|█▍        | 28/192 [20:25<2:07:39, 46.70s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  16%|█▌        | 30/192 [22:08<2:09:46, 48.07s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  17%|█▋        | 32/192 [23:19<1:58:26, 44.42s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  18%|█▊        | 34/192 [25:07<2:04:26, 47.26s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  19%|█▉        | 36/192 [26:32<1:58:56, 45.75s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  20%|█▉        | 38/192 [28:18<2:03:00, 47.92s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  21%|██        | 40/192 [29:36<1:54:56, 45.37s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  22%|██▏       | 42/192 [31:23<1:59:24, 47.76s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  23%|██▎       | 44/192 [32:44<1:52:32, 45.63s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  24%|██▍       | 46/192 [34:30<1:56:17, 47.79s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  25%|██▌       | 48/192 [35:50<1:49:02, 45.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  26%|██▌       | 50/192 [37:34<1:52:03, 47.35s/it]\u001b[A\u001b[A\n",
            "\n",
            "Grid Search Progress:  27%|██▋       | 52/192 [40:22<2:16:26, 58.47s/it]\u001b[A\u001b[A"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}