{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sting matching\n",
        "\n",
        "This Python code defines a function called simple_string_match that checks if one string (text2) is contained within another string (text1). The function uses the in operator to perform this check, returning True if text2 is found in text1, and False otherwise. The function includes a docstring explaining its purpose, parameters, and return value.\n",
        "The code then demonstrates how to use this function with an example. It creates two variables: text1 containing a full sentence, and text2 containing the word \"example\". It calls the simple_string_match function with these variables and uses an if-else statement to print whether the substring was found or not. This example illustrates basic string manipulation, function definition and usage, and conditional statements in Python."
      ],
      "metadata": {
        "id": "I-tMkacvl5KH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpnbIYYclio-",
        "outputId": "fc6b2dfb-e45e-4598-dfef-1b7bde0e1487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Substring found!\n"
          ]
        }
      ],
      "source": [
        "def simple_string_match(text1, text2):\n",
        "    \"\"\"\n",
        "    This function checks for the presence of a substring (text2) within a larger text (text1).\n",
        "\n",
        "    Args:\n",
        "        text1: The larger string to search within.\n",
        "        text2: The substring to search for.\n",
        "\n",
        "    Returns:\n",
        "        True if the substring is found, False otherwise.\n",
        "    \"\"\"\n",
        "    if text2 in text1:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Usage:\n",
        "text1 = \"This is an example sentence.\"\n",
        "text2 = \"example\"\n",
        "\n",
        "if simple_string_match(text1, text2):\n",
        "    print(\"Substring found!\")\n",
        "else:\n",
        "    print(\"Substring not found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance metrics\n",
        "\n",
        "This Python code defines a function called euclidean_distance that calculates the Euclidean distance between two text documents. It uses the CountVectorizer from scikit-learn to convert the text into numerical vectors based on word frequencies, and then applies the Euclidean distance formula from SciPy's spatial module. The function takes two text strings as input and returns a float representing their distance.\n",
        "The code then demonstrates the function's usage with an example. It creates two identical text strings and calculates their Euclidean distance using the defined function. The result is printed, which in this case would be 0.0 since the texts are identical.\n",
        "\n",
        "\n",
        "The Euclidean distance calculates the straight-line distance between two points in Euclidean space. It works by applying the Pythagorean theorem to a right triangle formed by the two points and their projections onto the axes.\n",
        "\n",
        "Here's how it works in two dimensions:\n",
        "\n",
        "- Consider two points:  Let's say we have point A with coordinates (x1, y1) and point B with coordinates (x2, y2).\n",
        "\n",
        "- Form a right triangle: Imagine drawing a horizontal line from A and a vertical line from B.  The point where these lines intersect (let's call it C) forms a right angle.\n",
        "\n",
        "- Calculate side lengths: The length of AC is the difference in x-coordinates (x2 - x1), and the length of BC is the difference in y-coordinates (y2 - y1).\n",
        "\n",
        "- Apply Pythagorean theorem:  The Euclidean distance between A and B is the hypotenuse of the triangle (AB).  Using the Pythagorean theorem (a² + b² = c²), we can calculate it as: distance AB = √[(x2 - x1)² + (y2 - y1)²]\n",
        "\n",
        "In higher dimensions, the Euclidean distance is simply an extension of this concept, incorporating the differences in additional coordinates (e.g., z-coordinates in three dimensions).\n",
        "\n",
        "**Analogy**\n",
        "\n",
        " You have a map with two points marked – your starting position and the location of the hidden treasure. The Euclidean distance is like measuring the shortest distance between your starting point and the treasure, as if you could walk in a straight line through everything in the park (trees, ponds, etc.).\n",
        "\n",
        "It's like using a ruler to find the length of that straight path, ignoring any obstacles or detours you might have to take in reality."
      ],
      "metadata": {
        "id": "gDDJBBkil_wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from scipy.spatial import distance\n",
        "\n",
        "def euclidean_distance(text1, text2):\n",
        "    \"\"\"\n",
        "    Calculates the Euclidean distance between two text documents.\n",
        "\n",
        "    Args:\n",
        "        text1: The first text document (string).\n",
        "        text2: The second text document (string).\n",
        "\n",
        "    Returns:\n",
        "        The Euclidean distance between the documents (float).\n",
        "    \"\"\"\n",
        "    vectorizer = CountVectorizer()\n",
        "    vectors = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "    # Convert sparse vectors to dense arrays and flatten them\n",
        "    vector1 = vectors[0].toarray().flatten()\n",
        "    vector2 = vectors[1].toarray().flatten()\n",
        "\n",
        "    return distance.euclidean(vector1, vector2)\n",
        "\n",
        "# Usage:\n",
        "text1 = \"This is an example sentence.\"\n",
        "text2 = \"This is an example sentence.\"\n",
        "\n",
        "dist = euclidean_distance(text1, text2)\n",
        "print(f\"Euclidean distance: {dist}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omPg0exvmRLu",
        "outputId": "710066a3-24a3-4558-9a78-6a624b7b7c50"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean distance: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embedding\n",
        "\n",
        "This Python code demonstrates the use of Word2Vec, a popular word embedding technique, using the Gensim library. It starts by defining a small dataset of tokenized sentences. The Word2Vec model is then trained on these sentences, creating vector representations for each word in the vocabulary.\n",
        "After training, the code retrieves the vector representations for two specific words: \"sentence\" and \"cats\". It then calculates the cosine similarity between these two word vectors using the model's built-in similarity function. Finally, it prints the vector representations of both words and their similarity score.\n"
      ],
      "metadata": {
        "id": "ikt0uvu_nDiG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cosine similarity** is a way to measure how similar two things are, especially when those things are represented as vectors (lists of numbers).  Imagine each vector as an arrow pointing in a certain direction in space. Cosine similarity focuses on the angle between these two arrows, rather than their lengths.\n",
        "\n",
        "Here's how it works:\n",
        "\n",
        "1. **Represent your items as vectors:**\n",
        "   Each item (e.g., a document, a word, an image) is converted into a vector, where each number in the vector represents a specific feature or attribute. For example, in text analysis, each number might represent the frequency of a particular word.\n",
        "\n",
        "2. **Calculate the dot product:**\n",
        "   The dot product is a mathematical operation that multiplies corresponding elements of the two vectors and then sums up the results. It gives you an idea of how much the two vectors \"overlap\" in terms of their direction.\n",
        "\n",
        "3. **Calculate the magnitudes:**\n",
        "   The magnitude of a vector is its length, calculated using the Pythagorean theorem (square root of the sum of squares of its components).\n",
        "\n",
        "4. **Find the cosine of the angle:**\n",
        "   Cosine similarity is the dot product of the two vectors divided by the product of their magnitudes.  This value corresponds to the cosine of the angle between the two vectors.\n",
        "\n",
        "The resulting cosine similarity value ranges from -1 to 1:\n",
        "\n",
        "* **1:** The vectors point in exactly the same direction (the angle between them is 0 degrees). This means the items are very similar.\n",
        "* **0:** The vectors are perpendicular (the angle between them is 90 degrees). This means the items are completely dissimilar.\n",
        "* **-1:** The vectors point in opposite directions (the angle between them is 180 degrees). This means the items are very dissimilar in an opposing way.\n",
        "\n",
        "**Analogy:**\n",
        "\n",
        "Imagine two flashlights shining on a wall. If they are shining in the same direction, the light overlaps completely (cosine similarity = 1). If they are shining at a right angle, there's no overlap (cosine similarity = 0). If they are shining in opposite directions, the light is completely separate (cosine similarity = -1).\n"
      ],
      "metadata": {
        "id": "p5Wn-vwEI-fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Sample sentences (your actual dataset would be much larger)\n",
        "sentences = [\n",
        "    [\"This\", \"is\", \"an\", \"example\", \"sentence\"],\n",
        "    [\"This\", \"is\", \"another\", \"sentence\"],\n",
        "    [\"Yet\", \"another\", \"sentence\", \"about\", \"cats\"],\n",
        "    [\"Cats\", \"are\", \"awesome\", \"pets\"]\n",
        "]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "model = Word2Vec(sentences, min_count=1)  # Train on all words (min_count=1)\n",
        "\n",
        "# Get word vectors\n",
        "word1 = \"sentence\"\n",
        "word2 = \"cats\"\n",
        "\n",
        "vector1 = model.wv[word1]\n",
        "vector2 = model.wv[word2]\n",
        "\n",
        "# Calculate similarity (cosine similarity is common for word embeddings)\n",
        "similarity = model.wv.similarity(word1, word2)\n",
        "\n",
        "print(f\"Vector for '{word1}': {vector1}\")\n",
        "print(f\"Vector for '{word2}': {vector2}\")\n",
        "print(f\"Similarity between '{word1}' and '{word2}': {similarity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5V-RJFVqG9G",
        "outputId": "ed845173-1c88-47f5-a831-4254db092ea6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector for 'sentence': [-5.3624064e-04  2.3643726e-04  5.1034773e-03  9.0094982e-03\n",
            " -9.3031824e-03 -7.1169869e-03  6.4590340e-03  8.9732129e-03\n",
            " -5.0155534e-03 -3.7634657e-03  7.3806890e-03 -1.5335097e-03\n",
            " -4.5367270e-03  6.5542157e-03 -4.8602819e-03 -1.8160631e-03\n",
            "  2.8766517e-03  9.9189859e-04 -8.2854219e-03 -9.4490545e-03\n",
            "  7.3119490e-03  5.0703888e-03  6.7578624e-03  7.6288462e-04\n",
            "  6.3510491e-03 -3.4054511e-03 -9.4642502e-04  5.7687177e-03\n",
            " -7.5218258e-03 -3.9362018e-03 -7.5117699e-03 -9.3006546e-04\n",
            "  9.5383571e-03 -7.3193498e-03 -2.3338271e-03 -1.9377895e-03\n",
            "  8.0776392e-03 -5.9310440e-03  4.5163568e-05 -4.7538527e-03\n",
            " -9.6037909e-03  5.0074183e-03 -8.7598041e-03 -4.3919352e-03\n",
            " -3.5100860e-05 -2.9618884e-04 -7.6614316e-03  9.6149836e-03\n",
            "  4.9821823e-03  9.2333741e-03 -8.1581213e-03  4.4959104e-03\n",
            " -4.1371793e-03  8.2455669e-04  8.4988326e-03 -4.4622882e-03\n",
            "  4.5176134e-03 -6.7871297e-03 -3.5485774e-03  9.3987426e-03\n",
            " -1.5776921e-03  3.2137960e-04 -4.1407333e-03 -7.6828799e-03\n",
            " -1.5080459e-03  2.4698565e-03 -8.8804914e-04  5.5338000e-03\n",
            " -2.7430458e-03  2.2601217e-03  5.4559307e-03  8.3461618e-03\n",
            " -1.4537770e-03 -9.2083728e-03  4.3706619e-03  5.7179929e-04\n",
            "  7.4420944e-03 -8.1330305e-04 -2.6384797e-03 -8.7532280e-03\n",
            " -8.5657829e-04  2.8266336e-03  5.4015638e-03  7.0528327e-03\n",
            " -5.7032639e-03  1.8588662e-03  6.0890159e-03 -4.7981711e-03\n",
            " -3.1073382e-03  6.7977994e-03  1.6315164e-03  1.8992183e-04\n",
            "  3.4737240e-03  2.1778293e-04  9.6190665e-03  5.0607305e-03\n",
            " -8.9176130e-03 -7.0417365e-03  9.0147840e-04  6.3926936e-03]\n",
            "Vector for 'cats': [-9.5785465e-03  8.9431154e-03  4.1650687e-03  9.2347348e-03\n",
            "  6.6435025e-03  2.9247368e-03  9.8040197e-03 -4.4246409e-03\n",
            " -6.8033109e-03  4.2273807e-03  3.7290000e-03 -5.6646108e-03\n",
            "  9.7047603e-03 -3.5583067e-03  9.5494064e-03  8.3472609e-04\n",
            " -6.3384566e-03 -1.9771170e-03 -7.3770545e-03 -2.9795230e-03\n",
            "  1.0416972e-03  9.4826873e-03  9.3558477e-03 -6.5958775e-03\n",
            "  3.4751510e-03  2.2755705e-03 -2.4893521e-03 -9.2291720e-03\n",
            "  1.0271263e-03 -8.1657059e-03  6.3201892e-03 -5.8000805e-03\n",
            "  5.5354391e-03  9.8337233e-03 -1.6000033e-04  4.5284927e-03\n",
            " -1.8094003e-03  7.3607611e-03  3.9400971e-03 -9.0103243e-03\n",
            " -2.3985039e-03  3.6287690e-03 -9.9568366e-05 -1.2012708e-03\n",
            " -1.0554385e-03 -1.6716016e-03  6.0495257e-04  4.1650953e-03\n",
            " -4.2527914e-03 -3.8336217e-03 -5.2816868e-05  2.6935578e-04\n",
            " -1.6880632e-04 -4.7855065e-03  4.3134023e-03 -2.1719194e-03\n",
            "  2.1035396e-03  6.6652300e-04  5.9696771e-03 -6.8423809e-03\n",
            " -6.8157101e-03 -4.4762576e-03  9.4358288e-03 -1.5918827e-03\n",
            " -9.4292425e-03 -5.4504158e-04 -4.4489228e-03  6.0000787e-03\n",
            " -9.5836855e-03  2.8590010e-03 -9.2528323e-03  1.2498009e-03\n",
            "  5.9991982e-03  7.3973476e-03 -7.6214634e-03 -6.0530235e-03\n",
            " -6.8384409e-03 -7.9183402e-03 -9.4990805e-03 -2.1254970e-03\n",
            " -8.3593250e-04 -7.2562015e-03  6.7870365e-03  1.1196196e-03\n",
            "  5.8288667e-03  1.4728665e-03  7.8936579e-04 -7.3681297e-03\n",
            " -2.1766580e-03  4.3210792e-03 -5.0853146e-03  1.1307895e-03\n",
            "  2.8833640e-03 -1.5363609e-03  9.9322954e-03  8.3496347e-03\n",
            "  2.4156666e-03  7.1182456e-03  5.8914376e-03 -5.5806171e-03]\n",
            "Similarity between 'sentence' and 'cats': 0.0270574651658535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained word embedding"
      ],
      "metadata": {
        "id": "Q00Sw3ukwMFJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code demonstrates the use of a pre-trained Word2Vec model from the Gensim library. It loads the 'word2vec-google-news-300' model, which was trained on Google News data and produces 300-dimensional word vectors. The code then defines a list of target words and uses the model to find the three most similar words for each target word based on their vector representations.\n",
        "For each target word, the code prints the top three most similar words along with their similarity scores. This similarity is based on the cosine similarity between word vectors in the embedding space."
      ],
      "metadata": {
        "id": "CMYCHv9JwJYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Load a pre-trained Word2Vec model\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Target words\n",
        "target_words = [\"king\", \"queen\", \"man\", \"woman\", \"cat\", \"dog\"]\n",
        "\n",
        "# Find similar words for each target word\n",
        "for word in target_words:\n",
        "    similar_words = model.most_similar(word, topn=3)  # Get top 3 most similar words\n",
        "\n",
        "    print(f\"\\nWords most similar to '{word}':\")\n",
        "    for similar_word, similarity in similar_words:\n",
        "        print(f\"- {similar_word}: {similarity:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbXEmsbDm-QT",
        "outputId": "689a18d7-6c91-496b-a219-8e71c4d53fc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "\n",
            "Words most similar to 'king':\n",
            "- kings: 0.714\n",
            "- queen: 0.651\n",
            "- monarch: 0.641\n",
            "\n",
            "Words most similar to 'queen':\n",
            "- queens: 0.740\n",
            "- princess: 0.707\n",
            "- king: 0.651\n",
            "\n",
            "Words most similar to 'man':\n",
            "- woman: 0.766\n",
            "- boy: 0.682\n",
            "- teenager: 0.659\n",
            "\n",
            "Words most similar to 'woman':\n",
            "- man: 0.766\n",
            "- girl: 0.749\n",
            "- teenage_girl: 0.734\n",
            "\n",
            "Words most similar to 'cat':\n",
            "- cats: 0.810\n",
            "- dog: 0.761\n",
            "- kitten: 0.746\n",
            "\n",
            "Words most similar to 'dog':\n",
            "- dogs: 0.868\n",
            "- puppy: 0.811\n",
            "- pit_bull: 0.780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Grams"
      ],
      "metadata": {
        "id": "diuJx-vNp0p1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Python code demonstrates the creation of n-grams, specifically bigrams and trigrams, from a given text string. The code starts by defining a sample sentence. It then splits this sentence into individual words and creates bigrams by pairing each word with the next word in the sequence. This process is repeated to create trigrams, where each group consists of three consecutive words.\n",
        "The code then prints out all the generated bigrams and trigrams. This technique is commonly used in natural language processing for tasks such as language modeling, text generation, and feature extraction. N-grams capture local word order and can provide insights into word associations and patterns within the text."
      ],
      "metadata": {
        "id": "z7jFh_FhwaJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"the quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "# Create bigrams (2-grams)\n",
        "bigrams = []\n",
        "words = text.split()\n",
        "for i in range(len(words)-1):\n",
        "    bigram = (words[i], words[i+1])  # Pair current word with the next\n",
        "    bigrams.append(bigram)\n",
        "\n",
        "print(\"Bigrams:\")\n",
        "for bigram in bigrams:\n",
        "    print(bigram)\n",
        "\n",
        "# Create trigrams (3-grams) – just expand the window\n",
        "trigrams = []\n",
        "for i in range(len(words)-2):\n",
        "    trigram = (words[i], words[i+1], words[i+2])\n",
        "    trigrams.append(trigram)\n",
        "\n",
        "print(\"\\nTrigrams:\")\n",
        "for trigram in trigrams:\n",
        "    print(trigram)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUtj4hPKod0i",
        "outputId": "b2b8917b-7cd9-4782-e5ba-172588fbba11"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams:\n",
            "('the', 'quick')\n",
            "('quick', 'brown')\n",
            "('brown', 'fox')\n",
            "('fox', 'jumps')\n",
            "('jumps', 'over')\n",
            "('over', 'the')\n",
            "('the', 'lazy')\n",
            "('lazy', 'dog')\n",
            "\n",
            "Trigrams:\n",
            "('the', 'quick', 'brown')\n",
            "('quick', 'brown', 'fox')\n",
            "('brown', 'fox', 'jumps')\n",
            "('fox', 'jumps', 'over')\n",
            "('jumps', 'over', 'the')\n",
            "('over', 'the', 'lazy')\n",
            "('the', 'lazy', 'dog')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_ngrams(text, n):\n",
        "    \"\"\"Generates n-grams (tuples of n consecutive words) from a given text.\"\"\"\n",
        "    words = text.split()  # Split the text into words\n",
        "    return zip(*[words[i:] for i in range(n)])  # Generate n-grams\n",
        "\n",
        "# Two texts to compare\n",
        "text1 = \"This is an original sentence.\"\n",
        "text2 = \"This is a partly reused sentence.\"\n",
        "\n",
        "# Generate bigrams (2-grams) and convert them to sets for easy comparison\n",
        "bigrams1 = set(generate_ngrams(text1, 2))\n",
        "bigrams2 = set(generate_ngrams(text2, 2))\n",
        "\n",
        "# Find common bigrams\n",
        "common_bigrams = bigrams1.intersection(bigrams2)\n",
        "\n",
        "print(\"Common Bigrams:\")  # Display common bigrams\n",
        "for bigram in common_bigrams:\n",
        "    print(bigram)\n",
        "\n",
        "# Calculate a simple similarity score based on shared bigrams\n",
        "similarity_score = len(common_bigrams) / max(len(bigrams1), len(bigrams2))\n",
        "print(f\"\\nSimilarity Score: {similarity_score:.2f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U97ifEsDrJGh",
        "outputId": "1c382191-3317-4381-b244-fef56b33f55e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Bigrams:\n",
            "('This', 'is')\n",
            "\n",
            "Similarity Score: 0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HASH\n",
        "\n",
        "This Python code demonstrates the use of hashing for text comparison. It defines a function hash_text that calculates a hash value for a given string using Python's built-in hash() function. The code then applies this function to three text strings: an original text, a plagiarized (identical) text, and a slightly modified text.\n"
      ],
      "metadata": {
        "id": "fiNMl0eVs2gx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hash_text(text):\n",
        "    \"\"\"Calculates the hash value of a given text.\"\"\"\n",
        "    return hash(text)\n",
        "\n",
        "# Example usage\n",
        "original_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "plagiarized_text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "modified_text = \"The quick brown fox jumps over the lazy cat.\"\n",
        "\n",
        "hash_original = hash_text(original_text)\n",
        "hash_plagiarized = hash_text(plagiarized_text)\n",
        "hash_modified = hash_text(modified_text)\n",
        "\n",
        "print(f\"Hash of Original Text (check the length): {hash_original}\")\n",
        "print(f\"Hash of Plagiarized Text (check the length): {hash_plagiarized}\")\n",
        "print(f\"Hash of Modified Text (check the length): {hash_modified}\")\n",
        "\n",
        "# Compare hashes\n",
        "if hash_original == hash_plagiarized:\n",
        "    print(\"Plagiarized Text is an exact copy of the Original Text.\")\n",
        "else:\n",
        "    print(\"Plagiarized Text is NOT an exact copy of the Original Text.\")\n",
        "\n",
        "if hash_original == hash_modified:\n",
        "    print(\"Modified Text is an exact copy of the Original Text.\")\n",
        "else:\n",
        "    print(\"Modified Text is NOT an exact copy of the Original Text.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RZmPqHstLxm",
        "outputId": "cfacb255-60ff-47f5-c5a2-037ee49e43b2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hash of Original Text (check the length): 6692867180466064543\n",
            "Hash of Plagiarized Text (check the length): 6692867180466064543\n",
            "Hash of Modified Text (check the length): 4220578042092582446\n",
            "Plagiarized Text is an exact copy of the Original Text.\n",
            "Modified Text is NOT an exact copy of the Original Text.\n"
          ]
        }
      ]
    }
  ]
}