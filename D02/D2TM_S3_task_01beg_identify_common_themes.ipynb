{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNehJttOwNbVUTsbapAbORl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Beginner"],"metadata":{"id":"BQOIp-kQ7o33"}},{"cell_type":"markdown","source":["## Task 01: Identify common themes in a corpus of Old English texts (or any other text)\n","\n","Task: Identify common themes in a corpus of Old English texts.\n","        Description: Use a simple topic model (e.g., LDA) to find recurring topics in a collection of Old English poems or prose.\n","        Hints:\n","        Preprocess the text by removing stopwords and lemmatizing.\n","        Start with a small number of topics (e.g., 5-10).\n","\n","\n","This guide will walk you through the code, explaining each part and helping you implement it.\n","\n","1. Setting Up the Environment:\n","\n","    NLTK: This code uses a library called NLTK for natural language processing tasks. You'll need to download some resources for NLTK to work. In the first section of the code, there are lines that start with nltk.download(). Run each of these lines one by one in your coding environment (like a Jupyter Notebook or a Python script) to download the necessary resources.\n","\n","2. Preparing Your Texts:\n","\n","    Sample Texts: The code includes some sample Old English texts. This is where you'll replace those samples with your actual texts you want to analyze. Each text should go inside the quotation marks within the texts list. Make sure to add commas to separate each entry in the list.\n","\n","3. Preprocessing Function:\n","\n","    Understanding the Function: This part of the code defines a function called preprocess that takes text as input and performs some cleaning tasks. Let's break down what it does:\n","    \n","    - Tokenization: It splits the text into smaller units like words using word_tokenize. Imagine cutting up a sentence into individual words.\n","    - Removing Punctuation and Numbers: It removes punctuation marks (like commas, periods) and numbers using loops and conditional statements\n","    - Removing Stopwords: It removes common words that don't carry much meaning (like \"the\", \"a\", \"is\") using stopwords from NLTK.\n","    - Lemmatization: It reduces words to their base form (e.g., \"running\" becomes \"run\") using a process called lemmatization.\n","    - Running the Function: You don't need to call this function manually. The code later applies this function to all your texts automatically.\n","\n","4. Processing Your Texts:\n","\n","    This part uses the preprocess function on each text in your list and stores the cleaned texts in a new list called processed_texts.\n","\n","5. Creating the Dictionary:\n","\n","    Understanding Dictionaries: The code creates a \"dictionary\" using the corpora.Dictionary function. This dictionary acts like a list that keeps track of all the unique words encountered in your texts and assigns them an ID number.\n","\n","6. Creating the Corpus:\n","\n","    Understanding Corpus: The code creates a \"corpus\" using the corpora.doc2bow function. Think of a corpus as a collection of documents (your texts) represented numerically based on the dictionary. Here, each document is converted to a list where each word is represented by its ID and its frequency (how many times it appears) in that document.\n","\n","7. Training the LDA Model:\n","\n","    LDA Model: This code uses a technique called Latent Dirichlet Allocation (LDA) to identify hidden topics in your texts. The gensim.models.LdaMulticore function creates the LDA model.\n","    Number of Topics: You can adjust the num_topics variable to specify how many hidden topics you want the model to find in your texts. By default, it's set to 5. Experiment with different values to see how it affects the results.\n","\n","8. Printing the Topics:\n","\n","    Understanding Topics: After training the model, the code uses the print_topics function to show the most relevant words for each of the identified topics. This helps you understand what each topic represents.\n","\n","9. Printing Dominant Topic per Text:\n","\n","    This part analyzes each of your texts and figures out the most dominant topic (out of the 5 identified) for each text. It then prints the text number, the dominant topic ID, and a score indicating how strong that topic is for that text.\n","       "],"metadata":{"id":"zRerVQ90tLgZ"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D56pTmCZ7WUF","executionInfo":{"status":"ok","timestamp":1719760475399,"user_tz":-120,"elapsed":13004,"user":{"displayName":"Tamas","userId":"07327650368274908773"}},"outputId":"a6dde89a-b13d-4f9e-fdf9-8c330320adc6"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"]},{"output_type":"stream","name":"stdout","text":["Top topics:\n","Topic: 0 \n","Words: 0.078*\"æþelingas\" + 0.078*\"ða\" + 0.077*\"gardena\" + 0.077*\"gefrunon\" + 0.077*\"geardagum\" + 0.076*\"hwæt\" + 0.075*\"fremedon\" + 0.075*\"hu\" + 0.073*\"þeodcyninga\" + 0.072*\"þrym\"\n","\n","Topic: 1 \n","Words: 0.045*\"monegum\" + 0.045*\"gardena\" + 0.045*\"scyld\" + 0.045*\"ellen\" + 0.045*\"mægþum\" + 0.045*\"egsode\" + 0.045*\"eorlas\" + 0.045*\"þrym\" + 0.045*\"scefing\" + 0.045*\"hwæt\"\n","\n","Topic: 2 \n","Words: 0.072*\"ellen\" + 0.069*\"þrym\" + 0.066*\"þeodcyninga\" + 0.063*\"hu\" + 0.062*\"fremedon\" + 0.061*\"hwæt\" + 0.059*\"geardagum\" + 0.059*\"gefrunon\" + 0.058*\"gardena\" + 0.057*\"ða\"\n","\n","Topic: 3 \n","Words: 0.078*\"scefing\" + 0.078*\"meodosetla\" + 0.078*\"oft\" + 0.078*\"ofteah\" + 0.078*\"sceaþena\" + 0.078*\"mægþum\" + 0.078*\"scyld\" + 0.078*\"þreatum\" + 0.078*\"eorlas\" + 0.078*\"monegum\"\n","\n","Topic: 4 \n","Words: 0.046*\"egsode\" + 0.045*\"þrym\" + 0.045*\"gefrunon\" + 0.045*\"hu\" + 0.045*\"geardagum\" + 0.045*\"ða\" + 0.045*\"fremedon\" + 0.045*\"mægþum\" + 0.045*\"eorlas\" + 0.045*\"sceaþena\"\n","\n","Text 1 - Dominant topic: 0 (Score: 0.93)\n","Text 2 - Dominant topic: 3 (Score: 0.93)\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import \"SOMETHING IS MISSING FROM HERE THAT HELPS TOKENIZING\"\n","from nltk.stem import WordNetLemmatizer\n","import \"THE PACKAGE NAME IS MISSING\"\n","from gensim import corpora\n","import string\n","\n","# Download necessary NLTK data\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Sample Old English texts (replace with your actual texts as the code will work)\n","texts = [\n","    \"Hwæt! We Gardena in geardagum, þeodcyninga, þrym gefrunon, hu ða æþelingas ellen fremedon.\",\n","    \"Oft Scyld Scefing sceaþena þreatum, monegum mægþum, meodosetla ofteah, egsode eorlas.\",\n","    # Add more texts here\n","]\n","\n","# Preprocessing function\n","def preprocess(text):\n","    # Tokenize\n","    tokens = word_tokenize(text.\"A KIND OF TRANSFORMATION IS MISSING FROM HERE\"())\n","\n","    # Remove punctuation and numbers\n","    tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]\n","\n","    # Remove stopwords (note: these are modern English stopwords)\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [\"SOMETHING for SOMETHING in SOMETHING\" if token not in stop_words]\n","\n","    # Lemmatize\n","    lemmatizer = WordNetLemmatizer()\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n","\n","    return tokens\n","\n","# Preprocess all texts\n","processed_texts = [preprocess(text) for text in texts]\n","\n","# Create dictionary\n","dictionary = corpora.Dictionary(processed_texts)\n","\n","# Create corpus\n","corpus = [dictionary.doc2bow(text) for text in processed_texts]\n","\n","# Train LDA model\n","num_topics = 5  # You can adjust this\n","lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n","\n","# Print topics\n","print(\"Top topics:\")\n","for idx, topic in lda_model.print_topics(-1):\n","    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n","\n","# Print dominant topic for each text\n","for i, corp in enumerate(corpus):\n","    top_topics = lda_model.get_document_topics(corp)\n","    top_topic = sorted(top_topics, key=lambda x: x[1], reverse=True)[0]\n","    print(f\"Text {i + 1} - Dominant topic: {top_topic[0]} (Score: {top_topic[1]:.2f})\")"]},{"cell_type":"markdown","source":["## Solution"],"metadata":{"id":"2e7ywGHTu2rr"}},{"cell_type":"markdown","source":["1: from nltk.tokenize import word_tokenize\n","\n","2: import gensim\n","\n","3: tokens = word_tokenize(text.lower())\n","\n","4: tokens = [token for token in tokens if token not in string.punctuation and not token.isdigit()]"],"metadata":{"id":"lg3Kz4zwu53x"}}]}