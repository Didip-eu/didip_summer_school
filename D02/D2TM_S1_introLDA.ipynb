{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "vdIIZYlS7er8"
      },
      "source": [
        "# Topic Modelling in Python\n",
        "\n",
        "\n",
        "\n",
        "This notebook guides you through all the steps required to create a topic model with Gensim LDA and NMF and demonstrates some visualisation options:\n",
        "- Heatmap\n",
        "- pyLDAvis\n",
        "- Wordclouds\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCEvEgf17er8"
      },
      "source": [
        "\n",
        "## Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect the google-Drive\n",
        "\n",
        "(as we did yesterday: First import google-drive, then change to the summerschool-directory)"
      ],
      "metadata": {
        "id": "zf01pJQknPJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqoXmoGlnfpf",
        "outputId": "416517b9-9503-46cd-e609-609c20c4e994"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/didip_ss/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpG9kKC_nxWT",
        "outputId": "8ddb806b-4771-470c-8289-e462a322b42e"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/didip_ss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you haven't done it this morning: Check for updates:"
      ],
      "metadata": {
        "id": "RFDCqYdhn4rj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6i1qCYsn30Y",
        "outputId": "990ad8e4-3f41-405f-e598-01771ec70b87"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import of Python-packages`\n",
        "\n",
        "We will use `nltk` and `spacy` for preprocessing and `gensim` for calculating the model. `glob` is for file management, `numpy` and `pandas` for structuring of the data. `matplotlib`, `seaborn` and `pyLDAvis` are for visualisation. `re` is for regular expressions (we will use it only for one minor task, i promise)\n",
        "\n",
        "We might need to install the `pyLDAvis`-package first, this is done with `pip`in the next line.\n"
      ],
      "metadata": {
        "id": "g6eQcxOnnJmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis --quiet"
      ],
      "metadata": {
        "id": "_gnfVMgJ7yH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EKIbfMM7er9"
      },
      "outputs": [],
      "source": [
        "import nltk, re, numpy as np\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "import spacy\n",
        "\n",
        "\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel, LdaMulticore, Nmf\n",
        "from gensim.models import CoherenceModel\n",
        "import gensim\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5scRCU6l7er9"
      },
      "source": [
        "### Import a Stopword list from nltk\n",
        "\n",
        "(Adjust the value for the language variable if required)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "6QH0EqEWJaCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otecBGZn7er-"
      },
      "outputs": [],
      "source": [
        "language = 'english'\n",
        "stopword = stopwords.words(language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsmhnLrS7er_"
      },
      "source": [
        "**Optional**: Inspect the stop word list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy9PNQD17er_"
      },
      "outputs": [],
      "source": [
        "print('\\n'.join(stopword))\n",
        "print(len(stopword))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJvL4D6m7esA"
      },
      "source": [
        "**Optional**: Extend stop word list:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8w9hKH87esB"
      },
      "outputs": [],
      "source": [
        "stopword = stopword + ['wordxyz','wordyyz']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Alternative**: load your own stop word list from file\n",
        "\n",
        "A txt file is expected as input, in which each word appears in a new line.\n",
        "\n",
        "We will load a stopword-list from our drive, it is in the `stopwords`-folder of the `data`-folder from `D02`. You can upload your own lists in that folder on your google-drive."
      ],
      "metadata": {
        "id": "9poCFZi69EQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./D02/data/stopwords/Stopwords_en.txt\",\"r\", encoding='utf8') as stopfile:\n",
        "   stopword = stopfile.read().splitlines()\n"
      ],
      "metadata": {
        "id": "vr2NIJly9Cvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional**: Inspect the stop word list"
      ],
      "metadata": {
        "id": "Jit4d3_yvRSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(stopword))"
      ],
      "metadata": {
        "id": "S9BCxioMvBo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXnNP05t7esB"
      },
      "source": [
        "## Text import and Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "\n",
        "In the following, we will write two different functions for preprocessing, one with `nltk`, one with `spacy`.\n",
        "\n",
        "In both settings, the texts are tokenised and all letters are converted to lower case, punctuation marks are removed and only the words that do not appear in the stop word list are included.\n",
        "\n",
        "For some applications, however, it is useful to consider only words with certain Part-of-Speech-Tags (POS-tags), e.g., only nouns. `spacy` is said to be faster ins POS-tagging, however, it is mostly fit for modern languages (at least as far as I know), therefore I also present the `nltk`-routine for older languages.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mCflYDKkMqBT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, first, the `nltk`-routine:\n",
        "\n",
        "The text is tokenized, converted to lower-case, only words consisting of alphabetic characters are kept, and finally only the words that do not appear in the stop word list are included in the output"
      ],
      "metadata": {
        "id": "ZRsK71jbbvFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nltk_prep(text):\n",
        "    ppwords = nltk.word_tokenize(text)\n",
        "    ppwords = [w.lower() for w in ppwords if w.isalpha()]\n",
        "    ppwords = [w for w in ppwords if w not in stopword]\n",
        "\n",
        "    return ppwords"
      ],
      "metadata": {
        "id": "n6YncVyh64Lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, the `spacy`-routine:\n",
        "\n",
        "We first have to load a language package (which should be changed of course, if you use other languages)."
      ],
      "metadata": {
        "id": "FR84VSuocMZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "TGFblsi9VFto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, spacy handles texts with maximum 1.000.000. Since we have some longer texts, we set the limit higher. Be warned, this might exceed your storage. There is a workaround at the end of the notebook, but it is a tiny bit more complicated."
      ],
      "metadata": {
        "id": "-0zOqDVjtFdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.max_length = 2000000"
      ],
      "metadata": {
        "id": "BVe_ICr6s8q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we do the preprocessing by converting the text to a spacy-object, `.text` provides the tokenized text, `.pos_` the POS-tags (don't forget the underscore at the end)"
      ],
      "metadata": {
        "id": "o4zKUuI6cY1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_prep(text):\n",
        "    doc = nlp(text)\n",
        "    ppword = [t.text for t in doc if t.text not in stopword and t.pos_==\"NOUN\"]\n",
        "    return(ppword)"
      ],
      "metadata": {
        "id": "wBngCLtuVF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hint**: In some scenarios, you even want to lemmatize the text. You can easily do that by changing `t.text` into `t.lemma_` in  the `spacy_prep` function"
      ],
      "metadata": {
        "id": "fO39XIVPzm-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see by an example, what the two routines do:\n",
        "\n"
      ],
      "metadata": {
        "id": "EX7u2zwGcxfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a nice example text 4 preprocessing, 4thewin!\"\n",
        "print(nltk_prep(text))\n",
        "print(spacy_prep(text))\n"
      ],
      "metadata": {
        "id": "00rCrSWnW48R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text import\n",
        "\n",
        "Now, we are ready import our texts and convert them into preprocessed tokens.\n",
        "\n",
        "First, we create two empty lists, `docs` for the tokenized text, `docnames` for the filenames of the documents. We fill these lists by looping through our uploaded files and sending their text to either one of our preprocessing routines."
      ],
      "metadata": {
        "id": "NYbbG5AfdMg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq87eUP-7esB"
      },
      "outputs": [],
      "source": [
        "docs=[]\n",
        "docnames=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "directory = './D02/data/corpus_of_english_fiction/'\n",
        "for filename in glob.glob(directory +\"*.txt\"):\n",
        "    with(open(filename)) as textfile:\n",
        "        text = textfile.read()\n",
        "\n",
        "    print(\"Import file \",filename,\" length of text: \", len(text))\n",
        "\n",
        "\n",
        "    ### Call nltk-preprocessing: Tokenize, keep only words that consist only of alphabet letters, and remove stopwords\n",
        "    wd = nltk_prep(text)\n",
        "\n",
        "    ### Alternative: Call Spacy-preprocessing: Tokenize, keep only words that consist only of alphabet letters, remove stopwords and keep only certain POS\n",
        "\n",
        "    #wd = spacy_prep(text)\n",
        "\n",
        "    print(\"Output \", len(wd),\" tokens\")\n",
        "    ### Append the output of the preprocessing routines to the docs-list\n",
        "    docs.append(wd)\n",
        "\n",
        "\n",
        "    ### Append the filename to the docnames-list\n",
        "    docnames.append(filename)\n",
        "\n",
        "print(\"Import done!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The previous import drew on a folder with single `txt`-files.\n",
        "\n",
        "If you want to import a `tsv`-file you could use (and adapt) the following routine that imports the monastery sample data,We convert the file into a dataframe and obtain our `docnames` from the column `atom_id`. Our text will be the english abstracts found in the column `translated_abstract_opus` and we will also make an additonal list `docdates` that will be filled with all the dates of the documents (because that could be handy for making a timeline).\n",
        "\n",
        "Note that in comparison to the previous cell (apart from the creation of the `docdates`-list) only the first three lines after the declaration of the empty lists are changed , the rest stays the same as above."
      ],
      "metadata": {
        "id": "VlsNFcZEHzGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs=[]\n",
        "docnames=[]\n",
        "docdates=[]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_df = pd.read_csv('./DATA/mom_1000_sample.tsv', sep='\\t')\n",
        "for doc,text,date in zip(text_df['atom_id'], text_df['translated_abstract_opus'],text_df['year']):\n",
        "    docname = doc\n",
        "\n",
        "\n",
        "    print(\"Import file \",docname,\" length of text: \", len(text))\n",
        "\n",
        "\n",
        "    ### Call nltk-preprocessing: Tokenize, keep only words that consist only of alphabet letters, and remove stopwords\n",
        "    wd = nltk_prep(text)\n",
        "\n",
        "    ### Alternative: Call Spacy-preprocessing: Tokenize, keep only words that consist only of alphabet letters, remove stopwords and keep only certain POS\n",
        "\n",
        "    #wd = spacy_prep(text)\n",
        "\n",
        "    print(\"Output \", len(wd),\" tokens\")\n",
        "    ### Append the output of the preprocessing routines to the docs-list\n",
        "    docs.append(wd)\n",
        "\n",
        "\n",
        "    ### Append the filename to the docnames-list\n",
        "    docnames.append(docname)\n",
        "\n",
        "    ### Append the year to the docdate-list\n",
        "    docdates.append(date)\n",
        "\n",
        "print(\"Import done!\")"
      ],
      "metadata": {
        "id": "1VLpXY5eFT5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5kPmHRX7esC"
      },
      "source": [
        "#### Inspect single preprocessed documents\n",
        "\n",
        "The preprocessed texts are located in `docs`, a list of lists. This will be the base for our model.\n",
        "\n",
        "**Optional:** To access the individual preprocessed texts, simply index docs. The following line outputs the third text (as usual in Python, the index count starts with 0):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq1tU5en7esC"
      },
      "outputs": [],
      "source": [
        "docs[2]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcOsJobW7esC"
      },
      "source": [
        "### Convert preprocessed documents into Bag-of-Words-model (BOW-model)\n",
        "\n",
        "The starting point for Gensim models is usually a bag-of-words model resp. a document term matrix, i.e. simply a list of all tokens and their frequency in the text document. The tokens are counted, `corpus` contains the tokens resolved into numbers, the assignment of the tokens to the indices is mapped in `dct`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASMNH_yR7esD"
      },
      "outputs": [],
      "source": [
        "dct = corpora.Dictionary(docs)\n",
        "corpus = [dct.doc2bow(line) for line in docs]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wmzQteC7esD"
      },
      "source": [
        "## Training of LDA-Model\n",
        "\n",
        "We will train our first model with `LdaMulticore`-from the `gensim`-package.\n",
        "\n",
        "Before training, we set the number of topics and store it in a variable (for convenience reasons). You can change the number here and experiment with different numbers of topics.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_topics = 20"
      ],
      "metadata": {
        "id": "ykOQ95Pk500t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train our model on the base of our BOW-corpus and our dictionary (This might take a while)\n",
        "\n",
        "Beside the input and `num_topics`, we have the following parameters:\n",
        "\n",
        "- `passes` specifies the number of training passes.\n",
        "- `chunksize` divides the texts before training into sections of a certain length (here preset to 500 tokens).\n",
        "\n",
        "Especially interesting to experiment with are the so called Priors, `alpha` und `eta`.\n",
        "- `alpha` determines how widely the topics are scattered across the documents (the higher the value for `alpha`, the more).\n",
        "- `eta` determines how widely the topics are scattered across the words.\n",
        "\n",
        "default-values for `alpha` und `eta` in Gensim are `1/num_topics`.\n"
      ],
      "metadata": {
        "id": "wWHnUTnT6RLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "GfcYiPmX7esD"
      },
      "outputs": [],
      "source": [
        "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
        "                                       id2word=dct,\n",
        "                                       num_topics = num_topics,\n",
        "                                       passes=70,\n",
        "                                       chunksize=500,\n",
        "                                       alpha = 1/num_topics,\n",
        "                                       eta = None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxW28sFp7esE"
      },
      "source": [
        "#### Exploration of Topics\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics and their most relevant words can be output with `print_topics()`"
      ],
      "metadata": {
        "id": "yBUQwB6Z65cO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HCky73N7esE"
      },
      "outputs": [],
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the most dominant topic for each text:"
      ],
      "metadata": {
        "id": "JJuZKJGj-hX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, corp in enumerate(corpus):\n",
        "    top_topics = lda_model.get_document_topics(corp)\n",
        "    top_topic = sorted(top_topics, key=lambda x: x[1], reverse=True)[0]\n",
        "    print(f\"Text {i + 1} - Dominant topic: {top_topic[0]} (Score: {top_topic[1]:.2f})\")"
      ],
      "metadata": {
        "id": "_UTbB3r5-mn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get topic distribution for each text:\n",
        "\n",
        "(We can use this to produce a heat map, see below)"
      ],
      "metadata": {
        "id": "IhpzwYzS5rOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, corp in enumerate(corpus):\n",
        "    top_topics = lda_model.get_document_topics(corp, minimum_probability=0)\n",
        "    print(re.sub(r'.*/(.*)\\.txt',r'\\1',docnames[i]))\n",
        "    print(top_topics)"
      ],
      "metadata": {
        "id": "lz1MUfig1xYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the relevance of a certain word for each topic"
      ],
      "metadata": {
        "id": "WQRcXqSC6kaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model.get_term_topics(\"thing\", minimum_probability=0)"
      ],
      "metadata": {
        "id": "Df5f8WjS6Sdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the representation for a single topic:"
      ],
      "metadata": {
        "id": "f7AQHwyg8k7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wot = lda_model.get_topic_terms(0, topn=15)\n",
        "for w in wot:\n",
        "    print(dct.get(w[0]), w[1])"
      ],
      "metadata": {
        "id": "RKbcUePO6-Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwphDmr57esE"
      },
      "source": [
        "### Evaluation of the model\n",
        "\n",
        "One way of evaluating the quality of topic models would be to check their coherence. Gensim has functions for this, see as follows. The higher the score, the more coherent the model is. The calculation takes a little time.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CIMEcp67esE"
      },
      "outputs": [],
      "source": [
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dct, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualization of the model"
      ],
      "metadata": {
        "id": "11s7DJGzHc8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization in Heatmap\n",
        "\n",
        "The heatmap shows the percentage of the topics for each document.\n",
        "\n",
        "We use the `get_document_topis` function as seen above to generate a dataframe and visualize it with a seaborn heatmap."
      ],
      "metadata": {
        "id": "76kE7n2UHedw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_dist=[]\n",
        "\n",
        "for corp in corpus:\n",
        "    top_dist = lda_model.get_document_topics(corp, minimum_probability=0)\n",
        "    t_dist.append([v[1] for v in top_dist])\n",
        "\n",
        "t_dist\n",
        "\n",
        "df = pd.DataFrame(t_dist, [re.sub(r'.*/(.*)\\.txt',r'\\1',n) for n in docnames])\n",
        "\n",
        "ax = sns.heatmap(df, linewidth=0.5)\n",
        "plt.yticks(rotation=0)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "_810YYHG_Rce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WZm6O_qeHa1w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPrjLze7esF"
      },
      "source": [
        "### Using the pyLDAvis package\n",
        "\n",
        "pyLDAvis is a visually attractive tool for displaying the topic distribution in a two-dimensional space (see below for more details on the visualisation).\n",
        "\n",
        "First, the output for Jupyter notebooks (such as this one) is activated, next the pyLDAvis model is prepared and stored in the variable `vis`, which can then simply be called:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W74MkiXA7esF"
      },
      "outputs": [],
      "source": [
        "pyLDAvis.enable_notebook()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvAdBALJ7esF"
      },
      "outputs": [],
      "source": [
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vIWyjskh7esF"
      },
      "outputs": [],
      "source": [
        "vis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Y88rGs7esG"
      },
      "source": [
        "Explanation of the visualization:\n",
        "\n",
        "Left side:\n",
        "\n",
        "pyLDAvis allocates the topics on a two-dimensional field, using Principal Component Analysis (PCA) to distribute the topics according to similarities in word usage.\n",
        "\n",
        "The larger the circle representing a topic, the greater its share of the texts in the corpus.\n",
        "\n",
        "Right side:\n",
        "\n",
        "The blue bars show the relative frequencies of the words in the entire corpus, the red bars (which appear when hovering over the topic circles with the mouse or when selecting them in the top left-hand navigation) show those in the respective topics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShQPRmG7esH"
      },
      "source": [
        "### Visualization in Wordclouds\n",
        "\n",
        "The following code is adapted from the tutorial https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
        "\n",
        "We create a table with Wordclouds for every topic, that visualize the most relevant words in a topic in a font size according to that relevance (thus, the largest word is the most relevant)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we import some packages:"
      ],
      "metadata": {
        "id": "VeGNzZn-miBy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCazffRJ7esH"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aFgFlUT7esH"
      },
      "source": [
        "First, we specify the number of topics to be displayed. We use the number of topics we generated in the model as default, but it is possible to also show less clouds than the total number of topics in the model.\n",
        "\n",
        "To modify, simply change the value of the variable. If necessary, however, it is also possible that the number of rows or columns must be adjusted below. The following assumes 20 topics, which are output in 4 rows of 5 word clouds (Feel free to write a routine that aumatically calculates the distribution of rows and columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6glKwLS7esH"
      },
      "outputs": [],
      "source": [
        "number_topics = num_topics\n",
        "table_cols = 5\n",
        "table_rows = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPMMYEF97esH"
      },
      "source": [
        "In the tutorial from which the code originates, each word cloud is displayed in a new colour. As this display is not always easy to read, I implemented a bit of a detour workaround, which initially sets black as the basic colour for all word clouds (a list is created in the size of the number of desired topics, for each of which the RGB value for black is specified. if you want a different colour, you can adjust the colour code `#000000` respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkQpiZR67esH"
      },
      "outputs": [],
      "source": [
        "cols = [\"#000000\"] * number_topics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nnuTM6c7esI"
      },
      "source": [
        "However, if you want a different coloured display as in the tutorial, simply remove the comment marker # in the following line and execute the line. (This makes the previous line of code obsolete).\n",
        "\n",
        "The colours are taken from the XKCD_COLORS colour list and can be changed, but there must be enough colours for the number of topics to be displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20osn5T47esI"
      },
      "outputs": [],
      "source": [
        "#cols = [color for name, color in mcolors.XKCD_COLORS.items()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoRtlKDw7esI"
      },
      "source": [
        "Next, the cloud is calculated.\n",
        "\n",
        "The parameters are largely self-explanatory and can be customised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvoq6jMj7esI"
      },
      "outputs": [],
      "source": [
        "cloud = WordCloud(stopwords=stopwords,\n",
        "                  background_color='white',\n",
        "                  width=250,\n",
        "                  height=180,\n",
        "                  max_words=15,\n",
        "                  colormap='Dark2',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl_JxHgA7esI"
      },
      "source": [
        "In the following line, the number of desired words can be adjusted under `num_word` (but be aware that the number should not exceed the value of `max_words` from the previous command that generates the cloud)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6TyCecv7esI"
      },
      "outputs": [],
      "source": [
        "topics = lda_model.show_topics(num_topics=number_topics, num_words= 15,formatted=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8BDGS9A7esJ"
      },
      "source": [
        "\n",
        "Finally, the table is pre-structured and plotted.\n",
        "\n",
        "In the for loop, the font size can be regulated with max_font_size.\n",
        "\n",
        "Within Python, the first topic has index 0, the second has index 1 and so on. For a more attractive display, 1 is added to each of the figure titles (str(i+1)).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpXZ7C0D7esJ"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(table_rows, table_cols, figsize=(10,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=400)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i+1), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IniLtPBo7esJ"
      },
      "source": [
        "#### Visualize a single topic in a Wordcloud\n",
        "\n",
        "`mytopic` contains the number of the topic (indexed with the internal count that starts at 0. For the title of the visualization, this count is increased by one).\n",
        "\n",
        "Thus, to display the word clouds for other topics, simply adjust the value of `mytopic`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZV7-H2UW7esJ"
      },
      "outputs": [],
      "source": [
        "mytopic=0\n",
        "\n",
        "topic_words = dict(topics[mytopic][1])\n",
        "cloud.generate_from_frequencies(topic_words, max_font_size=400)\n",
        "plt.gca().imshow(cloud)\n",
        "plt.gca().set_title('Topic ' + str(mytopic+1), fontdict=dict(size=16))\n",
        "plt.gca().axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other methods"
      ],
      "metadata": {
        "id": "esujwEnBKfKD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-Negative Matrix factorization\n",
        "\n",
        "Implementation with Gensim is as easy as LDA:"
      ],
      "metadata": {
        "id": "dvISiw9hKh4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_model = gensim.models.Nmf(corpus, num_topics = num_topics, id2word=dct, passes=70, chunksize=500)"
      ],
      "metadata": {
        "id": "efKKmO6nKy3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is calculated, we can use the same code as above. Just change the variable `lda_model` that we have used above to `nmf_model`."
      ],
      "metadata": {
        "id": "UZ6eZJU1yZhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance, we can show the topic with their most relevant words"
      ],
      "metadata": {
        "id": "DTXa_sdjy_mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, topic in nmf_model.print_topics(-1):\n",
        "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")"
      ],
      "metadata": {
        "id": "MWk3MlIUyhyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... or show the topic distribution"
      ],
      "metadata": {
        "id": "y1knhxjlz4oG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, corp in enumerate(corpus):\n",
        "    top_topics = nmf_model.get_document_topics(corp, minimum_probability=0)\n",
        "    print(re.sub(r'.*/(.*)\\.txt',r'\\1',docnames[i]))\n",
        "    print(top_topics)"
      ],
      "metadata": {
        "id": "_oOL1Zg1z5g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Leftovers"
      ],
      "metadata": {
        "id": "mA72Al7LWrYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split longer text into pieces for spacy\n"
      ],
      "metadata": {
        "id": "_9J-NfLEWtfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    if(len(text) < 1000000):\n",
        "        wd = spacy_prep(text)\n",
        "    else:\n",
        "        wd = []\n",
        "        for x in (range(int(len(text)/1000000))):\n",
        "            wd_temp = spacy_prep(text[x*1000000:x*1000000+999999])\n",
        "            print(x*1000000,x*1000000+999999)\n",
        "            wd.append(wd_temp)\n",
        "        wd_temp = spacy_prep(text[(x+1)*1000000:len(text)])\n",
        "        wd.append(wd_temp)"
      ],
      "metadata": {
        "id": "P8jp-JgWKrVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}