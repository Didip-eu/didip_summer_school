{"cells":[{"cell_type":"markdown","metadata":{},"source":["# How to train a NER model with Flair\n","\n","In this notebook, you will find all code that you need to train or finetune a language model and a NER model with the flairNLP framework.\n","\n","Note that this task is very computation-heavy and you will need a machine (server or local) with a CUDA-enabled video card.\n","\n","Another note: you can always about the execution of a cell or script to abort a model training. Flair will then quit and save the best current model for you."]},{"cell_type":"markdown","metadata":{},"source":["## Training a Language Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Make sure that flair is installed \n","%pip install -U flair"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# set the path to your project directory here (change this!)\n","root = \"./home/project/\""]},{"cell_type":"markdown","metadata":{},"source":["### Training Data\n","You will need a good amount of training data.\n","The data can simply be provided in txt-files\n","and if you may get a better model if you add additional segmentation information such as `[START]`, `[END]` and/or `[SEP]` to separate sample documents or sentences. \n","\n","For this example, we assume our training data is stored in the project folder in a folder `lm_data`. Inside this folder, Flair requires two files, `test.txt`, `valid.txt`, and one folder `train/`. The folder `train` can contain as many files with whatever names. The idea here is that training data for modern languages is usually so large you will store them in multiple files instead of a single one. If you have little amounts of data, splitting it in a 90/10/10 distribution is usually the way to go, but 70/15/15 may also work for you."]},{"cell_type":"markdown","metadata":{},"source":["#### Generating the character dictionary\n","You will need to create a custom dictionary for your dataset. Any character not included in this dictionary will be considered \"unknown\" by Flair. This doesn't break anything, but performance may decrease. If you don't have huge amounts of data, simply generate the dictionary from all files in your lm_data folder."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["files = [\n","    root + \"lm_data/test.txt\",\n","    root + \"lm_data/valid.txt\",\n","    root + \"lm_data/train/train.txt\",\n","]\n","\n","# we save the character dictionary simply to the project folder\n","out = root + 'char_dict.pkl'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# make an empty character dictionary\n","from flair.data import Dictionary\n","char_dictionary: Dictionary = Dictionary()\n","\n","# counter object\n","import collections\n","counter = collections.Counter()\n","\n","processed = 0\n","\n","for file in files:\n","    print(file)\n","\n","    with open(file, 'r', encoding='utf-8') as f:\n","        tokens = 0\n","        actual_tokens = 0\n","        for line in f:\n","\n","            processed += 1\n","            chars = list(line)\n","            tokens += len(chars)\n","\n","            # Add chars to the dictionary\n","            counter.update(chars)\n","\n","            # comment this line in to speed things up (if the corpus is too large)\n","            # if tokens > 50000000: break\n","\n","            tc = len([t for t in line.split() if t not in {\"[START]\", \"[END]\", \"[SEP]\"}])\n","\n","            actual_tokens += tc\n","\n","        print(actual_tokens)\n","\n","    # break\n","\n","total_count = 0\n","for letter, count in counter.most_common():\n","    total_count += count\n","\n","print(total_count)\n","print(processed)\n","\n","sum = 0\n","idx = 0\n","for letter, count in counter.most_common():\n","    sum += count\n","    percentile = (sum / total_count)\n","\n","    # comment this line in to use only top X percentile of chars, otherwise filter later\n","    # if percentile < 0.00001: break\n","\n","    char_dictionary.add_item(letter)\n","    idx += 1\n","    print('%d\\t%s\\t%7d\\t%7d\\t%f' % (idx, letter, count, sum, percentile))\n","\n","print(char_dictionary.item2idx)\n","\n","import pickle\n","with open(out, 'wb') as f:\n","    mappings = {\n","        'idx2item': char_dictionary.idx2item,\n","        'item2idx': char_dictionary.item2idx\n","    }\n","    pickle.dump(mappings, f)"]},{"cell_type":"markdown","metadata":{},"source":["Finally, we move on to the actual training. First the case without base model, training completely from scratch.\n","This training will train contextual character models for you and you should run it twice, once for forward and once for backward. Don't forget to change the parameters to do so!\n","\n","The training will run until the number of epochs is reached or until no improvements can be detected anymore."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#!/usr/bin/env python3\n","from flair.data import Dictionary\n","from flair.models import LanguageModel\n","from flair.trainers.language_model_trainer import LanguageModelTrainer, TextCorpus\n","\n","# True => forward, False => backward\n","is_forward_lm = True\n","\n","# we load our character dictionary that we created (change the path if necessary)\n","dictionary: Dictionary = Dictionary.load_from_file(root + 'char_dict.pkl')\n","\n","# create our corpus object (change the path if necessary)\n","corpus = TextCorpus(root + 'lm_data/',\n","                    dictionary,\n","                    is_forward_lm,\n","                    character_level=True)\n","\n","# instantiate your language model, set hidden size and number of layers (you can leave it as is)\n","language_model = LanguageModel(dictionary,\n","                            is_forward_lm,\n","                            hidden_size=2048,\n","                            nlayers=1)\n","\n","# prepare the trainer\n","trainer = LanguageModelTrainer(language_model, corpus)\n","\n","# actually run the trainer (change the output path if necessary)\n","trainer.train(root + 'lm_forward/',\n","            sequence_length=250,\n","            mini_batch_size=100,\n","            max_epochs=50)"]},{"cell_type":"markdown","metadata":{},"source":["### Fine-tuning a Language Model\n","Especially if you don't have huge amounts of training data, you may prefer fine-tuning your model on an already existing model instead.\n","\n","You can find base models in the [tutorial](https://flairnlp.github.io/docs/tutorial-embeddings/flair-embeddings) or on Huggingface."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from flair.embeddings import FlairEmbeddings\n","\n","# fetch the base model, here the \n","language_model = FlairEmbeddings('multi-forward-fast', has_decoder=True).lm\n","\n","# we can copy the forward/backward setting from the base model\n","is_forward_lm = language_model.is_forward_lm\n","\n","# we can use the character dictionary from the base model\n","dictionary: Dictionary = language_model.dictionary\n","\n","# set up the corpus object\n","corpus = TextCorpus(root + 'lm_data/',\n","                    dictionary,\n","                    is_forward_lm,\n","                    character_level=True)\n","\n","# instantiate your language model, set hidden size and number of layers\n","trainer = LanguageModelTrainer(language_model, corpus)\n","\n","# finetune your language model\n","trainer.train(root + 'lm_forward_finetuned/',\n","            sequence_length=250,\n","            mini_batch_size=100,\n","            learning_rate=20,\n","            patience=10,\n","            checkpoint=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Training a NER model\n","We will now train a model using the trained embeddings. You can of course also use others, Flair lets you stack a lot of different embedding types, also such as BERT. You can find publicly available embeddings in the Flair [tutorial](https://flairnlp.github.io/docs/tutorial-embeddings/flair-embeddings) but also on Huggingface.\n","\n","For this model, make sure you have three files containing your training, validation and test splits in IOB-annotated format.\n","In this example, we assume a column of words, one column or short NE-tags and one column of longer ne-tags.\n","\n","Flair also explains how to train your own model in their [tutorial](https://flairnlp.github.io/docs/tutorial-training/how-model-training-works). Among other things, they also [explain](https://flairnlp.github.io/docs/tutorial-training/how-to-train-sequence-tagger) there how to fine-tune BERT embeddings to annotate NER."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from flair.datasets import ColumnCorpus\n","from flair.embeddings import FlairEmbeddings, StackedEmbeddings\n","from flair.models import SequenceTagger\n","from flair.trainers import ModelTrainer\n","\n","columns = {0: 'text', 1: 'ner', 2: 'ner_long'}\n","\n","# set the path to the folder where train, validation and test files are in\n","data_folder = root + \"ner_data/\"\n","\n","# initialize the corpus object with the paths to train, test and validation (dev)\n","corpus: ColumnCorpus = ColumnCorpus(data_folder, columns,\n","                            train_file='train.txt',\n","                            test_file='test.txt',\n","                            dev_file='dev.txt')\n","\n","# If you have multiple label types, choose the labels you want to predict (from the names defined above in columns)\n","label_type = 'ner'\n","\n","# Create a dictionary with all labels in your NE-annotation\n","label_dict = corpus.make_label_dictionary(label_type=label_type, add_dev_test=True)\n","print(label_dict)\n","\n","# Put together the embeddings you want to use\n","embedding_types = [\n","    # you can add more embeddings here!\n","    FlairEmbeddings(root + \"lm_forward_finetuned/best-lm.pt\"),\n","    FlairEmbeddings(root + \"lm_backward_finetuned/best-lm.pt\"),\n","]\n","\n","# Combine the embeddings\n","embeddings = StackedEmbeddings(embeddings=embedding_types)\n","\n","# Initialize our sequence tagger\n","tagger = SequenceTagger(hidden_size=256,\n","                        embeddings=embeddings,\n","                        tag_dictionary=label_dict,\n","                        tag_type=label_type,\n","                        tag_format=\"BIO\"  # depends on your data\n","                        )\n","\n","# Initialize the trainer\n","trainer = ModelTrainer(tagger, corpus)\n","\n","# Start training (set the out path!)\n","trainer.train(root + \"ner_model/\",\n","            learning_rate=0.1,\n","            mini_batch_size=32,  # depending on data and gpu, reduce to 16, 8, 4, 2 or 1\n","            max_epochs=100)  # i chose many epochs here due to the small dataset i used for testing"]},{"cell_type":"markdown","metadata":{},"source":["Now let's test our model!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import the necessary parts of the library\n","from flair.data import Sentence\n","from flair.nn import Classifier\n","\n","# load the classifier from a local source by giving the path\n","tagger = Classifier.load(root + \"ner_model/\")\n","\n","# create a sentence object that will be predicted\n","sentence = Sentence(\"Galcherus Brideine , miles , dedit nobis decem solidos super aquam suam de Eschemilliaco ( b ) . Ego Galcherus Brideine , miles , notum facio universis presentes litteras inspecturis quod , cum bone memorie defunctus Ansellus Brideine , pater meus , in vita sua dedisset in perpetuam elemosinam pro salute anime sue ecclesie et fratribus Pontiniaci viginti solidos tur . percipiendos singulis annis in festo sancti Remigii in redditibus suis de Deveisel ( c ) , quos ipse emerat a domino Milone de Lynieres , ego , quem medietas dicte elemosine contingebat , assedi dictis Pontiniacensibus decem solidos tur . pro ( Ii ) parte mea super aquam meam de Eschemiliaco , volens et concedens ut quicumque predictam aquam de cetero tenuerit , predictos decem solidos singulis annis in octavis sancti Andree apostoli reddere dictis fratribus sine contradictione et diminutione imperpetuum ( 6 ) teneatur . Quod ut ratum et stabile permaneat in futurum , sigillum meum in testimonium et munimen duxi litteris presentibus apponendum . Actum anno Domini M ° cc ° LO VIIlo , mense aprili .\")\n","\n","# predict the annotations\n","tagger.predict(sentence)\n","\n","# show the annotations\n","for label in sentence.get_labels():\n","    print(label)"]},{"cell_type":"markdown","metadata":{},"source":["#### Fine-tuning a NER model\n","If you want to fine-tune a ner model, you can follow the same principles as an LM. We replace an initialized tagger with a base model, but you set the corpus and all up as usual. Note that this means you cannot define your own embeddings."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from flair.nn import Classifier\n","\n","# initialize the corpus like when training from scratch\n","\n","# load the base model\n","tagger = Classifier.load(\"ner-fast\")\n","\n","# Initialize the trainer\n","trainer = ModelTrainer(tagger, corpus)\n","\n","# Start training (set the out path!)\n","trainer.fine_tune(root + \"ner_model_finetuned/\")"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
