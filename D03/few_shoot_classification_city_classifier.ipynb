{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtoUeq0mYHhnsyF902hb2G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Advanced"],"metadata":{"id":"6gS2r-_W-kjz"}},{"cell_type":"markdown","source":["## City or not but it can be anything !!! (few-shot learning) classificator\n","\n","Absolutely! Let's break down this code step by step, providing a tutorial-style explanation.\n","\n","**Objective:**\n","\n","The core aim of this code is to build a model that can distinguish Latin city names from other Latin words (or non-city words). The model uses a pre-trained BERT (Bidirectional Encoder Representations from Transformers) language model for a \"few-shot learning\" scenario. In essence, we're teaching BERT to recognize a specific pattern (Latin city names) using a relatively small set of examples.\n","\n","**Code Structure:**\n","\n","1. **Setup:**\n","   - Import necessary libraries (PyTorch, Transformers, scikit-learn).\n","   - Load the pre-trained BERT model and its tokenizer.\n","\n","2. **Data Preparation:**\n","   - `latin_cities`: A list of Latin city names (positive examples).\n","   - `non_cities`: A list of other Latin words (negative examples).\n","   - Combine the data, create labels (1 for city, 0 for non-city), and shuffle.\n","   - Split the data into training, validation, and test sets.\n","   - `LatinWordDataset`: A custom class to load and tokenize the words for BERT.\n","\n","3. **Model Definition:**\n","   - `LatinCityClassifier`: A custom neural network class.\n","     - Leverages the pre-trained BERT model.\n","     - Adds a dropout layer (for regularization) and a linear classifier layer to make the final prediction (city or not city).\n","     \n","   - `prototypical_loss`: This function calculates the loss to update the model's learning. It uses a prototype-based approach, which means it tries to group similar examples together (city names close to other city names) while keeping them distinct from non-city words.\n","\n","4. **Training and Validation:**\n","   - Set up the training environment (device - CPU or GPU).\n","   - Define the optimizer (AdamW) and loss function (cross-entropy).\n","   - Iterate over training epochs.\n","     - In each epoch, calculate the loss and update the model's parameters based on the training data.\n","     - Evaluate the model's performance on the validation set and save the best-performing model.\n","\n","5. **Final Testing:**\n","   - Load the best-performing model from the validation phase.\n","   - Evaluate the model on the test set and print the accuracy, precision, recall, and F1 score.\n","\n","**Few-Shot Learning Adaptations:**\n","\n","This code specifically addresses a few-shot learning scenario through these modifications:\n","\n","- **Small Dataset:** The training dataset is deliberately limited.\n","- **Regularization:** A higher dropout rate (0.5) is used to prevent overfitting to the limited training data.\n","- **Prototypical Loss:**  This loss function is well-suited for few-shot learning as it focuses on creating distinct clusters (prototypes) for the different classes (city vs. non-city) in the feature space.\n","\n","**Key Points:**\n","\n","- **Transfer Learning:** By using a pre-trained BERT model, we're leveraging its existing knowledge of language, which helps significantly when you have limited training data.\n","- **Custom Dataset Class:** This simplifies the process of loading and preparing your specific data for the BERT model.\n","- **Regularization:** Dropout helps the model generalize better to unseen data.\n","\n","Let me know if you have any specific questions or would like me to elaborate on any aspect of the code!\n","\n"],"metadata":{"id":"1UmWypbJ-N0R"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dnAu9kWjpPFo","executionInfo":{"status":"ok","timestamp":1720014310547,"user_tz":-120,"elapsed":951869,"user":{"displayName":"Tamas","userId":"07327650368274908773"}},"outputId":"45485316-e59d-4f9b-cce4-bd19031c32ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/8, Train Loss: 0.6793, Val Loss: 0.6906, Accuracy: 0.5000, Precision: 0.3333, Recall: 1.0000, F1: 0.5000\n","Epoch 2/8, Train Loss: 0.6884, Val Loss: 0.6803, Accuracy: 0.3333, Precision: 0.2727, Recall: 1.0000, F1: 0.4286\n","Epoch 3/8, Train Loss: 0.5061, Val Loss: 0.5625, Accuracy: 0.8333, Precision: 0.6000, Recall: 1.0000, F1: 0.7500\n","Epoch 4/8, Train Loss: 0.4399, Val Loss: 0.4546, Accuracy: 0.9167, Precision: 0.7500, Recall: 1.0000, F1: 0.8571\n","Epoch 5/8, Train Loss: 0.3179, Val Loss: 0.3016, Accuracy: 0.9167, Precision: 0.7500, Recall: 1.0000, F1: 0.8571\n","Epoch 6/8, Train Loss: 0.2066, Val Loss: 0.1375, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n","Epoch 7/8, Train Loss: 0.0819, Val Loss: 0.1012, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n","Epoch 8/8, Train Loss: 0.0411, Val Loss: 0.0348, Accuracy: 1.0000, Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n","Final Test Accuracy: 0.9231, Precision: 1.0000, Recall: 0.8889, F1: 0.9412\n"]}],"source":["import torch\n","import random\n","from torch import nn\n","from transformers import AutoTokenizer, AutoModel\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","import numpy as np\n","\n","# Load pre-trained model and tokenizer\n","model_name = \"bert-base-multilingual-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","bert_model = AutoModel.from_pretrained(model_name)\n","\n","# Latin city names dataset\n","latin_cities = [\n","    \"Roma\", \"Athenae\", \"Alexandria\", \"Carthago\", \"Constantinopolis\",\n","    \"Neapolis\", \"Mediolanum\", \"Lugdunum\", \"Vindobona\", \"Antiochia\",\n","    \"Hierosolyma\", \"Damascus\", \"Tyrus\", \"Thessalonica\", \"Byzantium\",\n","    \"Sparta\", \"Corinth\", \"Lutetia\", \"Londinium\", \"Eboracum\",\n","    \"Augusta Treverorum\", \"Brundisium\", \"Toletum\", \"Florentia\", \"Syracusae\",\n","    \"Ephesus\", \"Pergamum\", \"Aquileia\", \"Massilia\", \"Tarraco\",\n","    \"Caesaraugusta\", \"Corduba\", \"Emerita Augusta\", \"Gades\", \"Pompeii\",\n","    \"Herculaneum\", \"Ravenna\", \"Nicomedia\", \"Mediolanum Santonum\", \"Colonia Agrippina\"\n","]\n","\n","\n","# Non-city words for negative examples (more diverse)\n","non_cities = [\n","    \"amo\", \"laudo\", \"dico\", \"habeo\", \"audio\", \"venio\", \"capio\", \"vivo\",\n","    \"scribo\", \"canto\", \"bellum\", \"pax\", \"sol\", \"luna\", \"terra\",\n","    \"aqua\", \"ignis\", \"aer\", \"arbor\", \"flumen\", \"mons\", \"mare\", \"coelum\", \"animal\", \"Caesar\", \"Cicero\",\n","    \"Augustus\", \"Vergilius\", \"Livius\", \"Caesar\", \"Cicero\", \"Augustus\", \"Vergilius\",\n","    \"Livius\",  \"et\", \"sed\", \"aut\", \"vel\", \"non\", \"si\", \"quia\", \"ut\",\n","]\n","\n","# Combine datasets and create labels\n","data = [(city, 1) for city in latin_cities] + [(word, 0) for word in non_cities]\n","random.shuffle(data)\n","\n","# Modify the data split\n","train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n","val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n","\n","# Create datasets and dataloaders\n","train_dataset = LatinWordDataset(train_data, tokenizer)\n","val_dataset = LatinWordDataset(val_data, tokenizer)\n","test_dataset = LatinWordDataset(test_data, tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=4)\n","test_loader = DataLoader(test_dataset, batch_size=4)\n","\n","# Custom dataset class\n","class LatinWordDataset(Dataset):\n","    def __init__(self, data, tokenizer, max_length=128):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        word, label = self.data[idx]\n","        encoding = self.tokenizer(word, return_tensors='pt', max_length=self.max_length, padding='max_length', truncation=True)\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        }\n","\n","# Modified model for few-shot learning\n","class LatinCityClassifier(nn.Module):\n","    def __init__(self, bert_model):\n","        super(LatinCityClassifier, self).__init__()\n","        self.bert = bert_model\n","        self.dropout = nn.Dropout(0.5)  # Higher dropout for regularization\n","        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output  # Use pooler output\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        return logits\n","\n","# Prototypical loss function\n","def prototypical_loss(prototype, query, labels):\n","    distances = torch.cdist(query, prototype, p=2.0)\n","    closest_prototype_index = distances.argmin(dim=1)  # Get closest prototype index for each query\n","    loss_val = nn.CrossEntropyLoss()(distances, closest_prototype_index) # Calculate cross-entropy loss\n","    return loss_val\n","\n","# Create datasets and dataloaders (smaller batch size for few-shot)\n","train_dataset = LatinWordDataset(train_data, tokenizer)\n","test_dataset = LatinWordDataset(test_data, tokenizer)\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=4)\n","\n","# Initialize model and optimizer\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = LatinCityClassifier(bert_model).to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training loop with validation\n","num_epochs = 8\n","best_val_acc = 0\n","patience = 3\n","epochs_without_improvement = 0\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    train_loss = 0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss += loss.item()\n","\n","    train_loss /= len(train_loader)\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0\n","    all_val_labels = []\n","    all_val_preds = []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            _, predicted = torch.max(outputs, 1)\n","            all_val_labels.extend(labels.cpu().numpy())\n","            all_val_preds.extend(predicted.cpu().numpy())\n","\n","    val_loss /= len(val_loader)\n","    val_acc = accuracy_score(all_val_labels, all_val_preds)\n","    val_precision = precision_score(all_val_labels, all_val_preds)\n","    val_recall = recall_score(all_val_labels, all_val_preds)\n","    val_f1 = f1_score(all_val_labels, all_val_preds)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n","          f\"Accuracy: {val_acc:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1: {val_f1:.4f}\")\n","\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), 'best_model_few_shot.pt')\n","        epochs_without_improvement = 0\n","    else:\n","        epochs_without_improvement += 1\n","        if epochs_without_improvement >= patience:\n","            print(\"Early stopping triggered!\")\n","            break\n","\n","# Load the best model for final evaluation\n","model.load_state_dict(torch.load('best_model_few_shot.pt'))\n","model.eval()\n","\n","all_test_labels = []\n","all_test_preds = []\n","with torch.no_grad():\n","    for batch in test_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","        all_test_labels.extend(labels.cpu().numpy())\n","        all_test_preds.extend(predicted.cpu().numpy())\n","\n","accuracy = accuracy_score(all_test_labels, all_test_preds)\n","precision = precision_score(all_test_labels, all_test_preds)\n","recall = recall_score(all_test_labels, all_test_preds)\n","f1 = f1_score(all_test_labels, all_test_preds)\n","print(f\"Final Test Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")"]},{"cell_type":"code","source":["# Define a function to preprocess and predict\n","def predict_word(word, model, tokenizer, device):\n","    model.eval()\n","    with torch.no_grad():\n","        inputs = tokenizer(word, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n","        input_ids = inputs['input_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","        return predicted.item()\n","\n","# Load the saved model\n","model = LatinCityClassifier(bert_model).to(device)\n","model.load_state_dict(torch.load('best_model_few_shot.pt'))\n","\n","# List of unseen words\n","unseen_words = [\"Tarraco\", \"Pompeii\", \"Marcus\", \"Cicero\", \"Senatus\", \"amo\", \"ego\", \"budapest\", \"szeged\", \"Wien\", \"vienna\", \"Graz\"]\n","\n","# Make predictions\n","for word in unseen_words:\n","    prediction = predict_word(word, model, tokenizer, device)\n","    print(f\"Word: {word}, Prediction: {'City' if prediction == 1 else 'Not a city'}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iNgWqT4UtlNc","executionInfo":{"status":"ok","timestamp":1720014531122,"user_tz":-120,"elapsed":8950,"user":{"displayName":"Tamas","userId":"07327650368274908773"}},"outputId":"b7fd0218-8611-4399-c301-38d281ec696b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word: Tarraco, Prediction: City\n","Word: Pompeii, Prediction: City\n","Word: Marcus, Prediction: Not a city\n","Word: Cicero, Prediction: Not a city\n","Word: Senatus, Prediction: Not a city\n","Word: amo, Prediction: Not a city\n","Word: ego, Prediction: Not a city\n","Word: budapest, Prediction: City\n","Word: szeged, Prediction: City\n","Word: Wien, Prediction: Not a city\n","Word: vienna, Prediction: City\n","Word: Graz, Prediction: City\n"]}]}]}